# DPA æ™ºèƒ½çŸ¥è¯†å¼•æ“ - æŠ€æœ¯è§„æ ¼æ–‡æ¡£ (TECH_SPEC)

> **ç‰ˆæœ¬**: v0.8 
> **æ›´æ–°æ—¥æœŸ**: 2025å¹´7æœˆ4æ—¥  
> **çŠ¶æ€**: MVPé˜¶æ®µ - 80%å®Œæˆåº¦  
> **æŠ€æœ¯æ ˆ**: Python 3.11.5 + LangChain 0.3.26 + LangGraph 0.4.8 + FastAPI 0.115.13  
> **å¼€å‘ç¯å¢ƒ**: dpa_gen condaç¯å¢ƒï¼Œè¿æ¥rtx4080æœåŠ¡å™¨é›†ç¾¤

## 1. ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ

### 1.1 æ™ºèƒ½çŸ¥è¯†å¼•æ“ç³»ç»Ÿæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  å‰ç«¯ç•Œé¢å±‚ (Frontend Layer)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ç ”ç©¶å·¥ä½œå°   â”‚ â”‚çŸ¥è¯†å›¾è°±     â”‚ â”‚é¡¹ç›®ç®¡ç†     â”‚ â”‚æŠ¥å‘Šç”Ÿæˆ   â”‚   â”‚
â”‚  â”‚(Next.js 14) â”‚ â”‚å¯è§†åŒ–       â”‚ â”‚ä»ªè¡¨æ¿       â”‚ â”‚å¯¼å‡º       â”‚   â”‚
â”‚  â”‚TypeScript   â”‚ â”‚(D3.js)      â”‚ â”‚(React)      â”‚ â”‚(PDF/MD)   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                   APIç½‘å…³å±‚ (API Gateway Layer)                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚èº«ä»½è®¤è¯     â”‚ â”‚é¡¹ç›®éš”ç¦»     â”‚ â”‚APIè·¯ç”±      â”‚ â”‚å®æ—¶é€šä¿¡   â”‚   â”‚
â”‚  â”‚(JWT)        â”‚ â”‚(RBAC)       â”‚ â”‚(FastAPI)    â”‚ â”‚(WebSocket)â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                   LangGraphæ™ºèƒ½ä½“å±‚ (Agent Layer)               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚æ–‡æ¡£å¤„ç†     â”‚ â”‚ç ”ç©¶è§„åˆ’     â”‚ â”‚çŸ¥è¯†é—®ç­”     â”‚ â”‚è®°å¿†ç®¡ç†   â”‚   â”‚
â”‚  â”‚æ™ºèƒ½ä½“       â”‚ â”‚æ™ºèƒ½ä½“       â”‚ â”‚æ™ºèƒ½ä½“       â”‚ â”‚æ™ºèƒ½ä½“     â”‚   â”‚
â”‚  â”‚(LangGraph)  â”‚ â”‚(LangGraph)  â”‚ â”‚(LangGraph)  â”‚ â”‚(LangGraph)â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                   LangChainå·¥å…·å±‚ (Tools Layer)                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚æ–‡æ¡£è§£æå·¥å…· â”‚ â”‚å‘é‡æ£€ç´¢å·¥å…· â”‚ â”‚å›¾æŸ¥è¯¢å·¥å…·   â”‚ â”‚å¤–éƒ¨API   â”‚   â”‚
â”‚  â”‚(LangChain)  â”‚ â”‚(LangChain)  â”‚ â”‚(LangChain)  â”‚ â”‚å·¥å…·       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                æ•°æ®å­˜å‚¨å±‚ (Storage Layer - rtx4080)             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚PostgreSQL   â”‚ â”‚Qdrant       â”‚ â”‚Neo4j        â”‚ â”‚Redis      â”‚   â”‚
â”‚  â”‚:5432        â”‚ â”‚:6333        â”‚ â”‚:7687        â”‚ â”‚:6379      â”‚   â”‚
â”‚  â”‚é¡¹ç›®å…ƒæ•°æ®   â”‚ â”‚å‘é‡ç´¢å¼•     â”‚ â”‚çŸ¥è¯†å›¾è°±     â”‚ â”‚ç¼“å­˜/ä¼šè¯  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æ ¸å¿ƒè®¾è®¡åŸåˆ™

- **LangGraphä¼˜å…ˆ**: æ‰€æœ‰å¤æ‚å·¥ä½œæµå‡åŸºäºLangGraphçŠ¶æ€æœºå®ç°
- **LangChainæ·±åº¦é›†æˆ**: å……åˆ†åˆ©ç”¨LangChainçš„å·¥å…·ç”Ÿæ€å’ŒRAGèƒ½åŠ›
- **å±‚æ¬¡åŒ–çŸ¥è¯†ç´¢å¼•**: å¤šç»´åº¦ã€å¤šå±‚æ¬¡çš„çŸ¥è¯†ç»„ç»‡å’Œæ£€ç´¢
- **æ¸è¿›å¼å­¦ä¹ **: é¡¹ç›®è®°å¿†åº“æ”¯æŒæŒç»­å­¦ä¹ å’ŒçŸ¥è¯†æ¼”åŒ–
- **ç²¾å‡†æº¯æº**: æ¯ä¸ªå›ç­”éƒ½èƒ½å‡†ç¡®è¿½æº¯åˆ°åŸå§‹æ–‡æ¡£ä½ç½®
- **æ¨¡å—åŒ–è®¾è®¡**: é«˜å†…èšã€ä½è€¦åˆçš„ç»„ä»¶åŒ–æ¶æ„
- **ç”Ÿäº§å°±ç»ª**: åŸºäºdpa_genç¯å¢ƒçš„ç¨³å®šéƒ¨ç½²é…ç½®

### 1.3 æŠ€æœ¯æ ˆé€‰å‹ä¸ç‰ˆæœ¬

#### 1.3.1 æ ¸å¿ƒæ¡†æ¶ (å·²é…ç½®å®Œæˆ âœ…)
- **LangGraph**: 0.4.8 - æ™ºèƒ½ä½“å·¥ä½œæµç¼–æ’å¼•æ“
- **LangChain**: 0.3.26 - RAGå·¥å…·é“¾å’Œæ¨¡å‹é›†æˆ
  - langchain-community: 0.3.26
  - langchain-core: 0.3.66
  - langchain-openai: 0.3.24
- **LangSmith**: 0.4.1 - å¯è§‚æµ‹æ€§å’Œè°ƒè¯•
- **FastAPI**: 0.115.13 - é«˜æ€§èƒ½å¼‚æ­¥Webæ¡†æ¶
- **Pydantic**: 2.10.3 - æ•°æ®éªŒè¯å’Œåºåˆ—åŒ–

#### 1.3.2 AI/MLç»„ä»¶ (å·²é…ç½®å®Œæˆ âœ…)
- **Embeddingæ¨¡å‹**: 
  - OpenAI text-embedding-3-large (ä¸»è¦)
  - BGE-M3 (å¤‡é€‰ï¼Œæœ¬åœ°éƒ¨ç½²)
- **è¯­è¨€æ¨¡å‹**: 
  - OpenAI GPT-4o (ä¸»åŠ›)
  - Anthropic Claude-3.5-Sonnet (å¤‡é€‰)
  - DeepSeek-V3 (æˆæœ¬ä¼˜åŒ–)
  - Cohere Command-R+ (ç‰¹å®šåœºæ™¯)
- **å‘é‡æ•°æ®åº“**: Qdrant 1.14.3+ (æ”¯æŒæ··åˆæ£€ç´¢)
- **å›¾æ•°æ®åº“**: Neo4j 5.28.1 (çŸ¥è¯†å›¾è°±å­˜å‚¨)

#### 1.3.3 æ•°æ®å­˜å‚¨ (rtx4080æœåŠ¡å™¨é›†ç¾¤ âœ…)
- **å…³ç³»æ•°æ®åº“**: PostgreSQL 16+ (é¡¹ç›®å…ƒæ•°æ®)
  - è¿æ¥åœ°å€: rtx4080:5432
  - æ•°æ®åº“: dpa_dev, dpa_test, dpa_prod
- **å‘é‡æ•°æ®åº“**: Qdrant (æ–‡æ¡£å‘é‡ç´¢å¼•)
  - è¿æ¥åœ°å€: rtx4080:6333
  - é›†åˆ: documents, chunks, conversations
- **å›¾æ•°æ®åº“**: Neo4j (çŸ¥è¯†å›¾è°±)
  - è¿æ¥åœ°å€: rtx4080:7687
  - æ•°æ®åº“: neo4j
- **ç¼“å­˜ç³»ç»Ÿ**: Redis (ä¼šè¯å’Œç¼“å­˜ç®¡ç†)
  - è¿æ¥åœ°å€: rtx4080:6379
  - æ•°æ®åº“: 0-15 (å¤šç”¨é€”éš”ç¦»)

#### 1.3.4 å¼€å‘å·¥å…·é“¾ (å·²é…ç½®å®Œæˆ âœ…)
- **Pythonç¯å¢ƒ**: 3.11.5 (dpa_gen condaç¯å¢ƒ)
- **åŒ…ç®¡ç†**: conda + pip (environment.yml + requirements.txt)
- **ä»£ç è´¨é‡**: 
  - Ruff 0.8.9+ (æ ¼å¼åŒ–å’Œlinting)
  - MyPy 1.13.0+ (ç±»å‹æ£€æŸ¥)
  - Bandit 1.8.0+ (å®‰å…¨æ‰«æ)
- **æµ‹è¯•æ¡†æ¶**: 
  - Pytest 8.3.4+
  - Pytest-asyncio 0.25.0+
  - è¦†ç›–ç‡ç›®æ ‡: >85%

#### 1.3.5 æ–‡æ¡£å¤„ç† (å·²å®ç° âœ…)
- **PDFè§£æ**: PyPDF 5.6.1
- **Wordæ–‡æ¡£**: python-docx 1.2.0
- **Markdown**: markdown 3.8.2
- **HTMLè§£æ**: BeautifulSoup4 4.13.4 + lxml 5.4.0
- **æ–‡æœ¬åˆ†å—**: LangChain RecursiveCharacterTextSplitter
- **è¯­ä¹‰åˆ†å—**: LangChain SemanticChunker (å®éªŒæ€§)

#### 1.3.6 å‰ç«¯æŠ€æœ¯ (è§„åˆ’ä¸­ ğŸ”„)
- **æ¡†æ¶**: Next.js 14 + TypeScript
- **UIç»„ä»¶**: shadcn/ui + Tailwind CSS
- **å¯è§†åŒ–**: D3.js + React Flow (çŸ¥è¯†å›¾è°±)
- **çŠ¶æ€ç®¡ç†**: Zustand + React Query
- **å®æ—¶é€šä¿¡**: Socket.io (WebSocket)

## 2. åŸºäºLangGraphçš„æ™ºèƒ½ä½“æ¶æ„

### 2.1 æ–‡æ¡£å¤„ç†æ™ºèƒ½ä½“ (Document Processing Agent)

#### 2.1.1 LangGraphå·¥ä½œæµå®šä¹‰
```python
from langgraph import StateGraph, END
from langchain_core.tools import BaseTool
from typing import TypedDict, List

class DocumentProcessingState(TypedDict):
    """æ–‡æ¡£å¤„ç†çŠ¶æ€"""
    project_id: str
    document_path: str
    document_type: str
    raw_content: str
    parsed_content: dict
    chunks: List[dict]
    embeddings: List[List[float]]
    entities: List[dict]
    knowledge_graph: dict
    processing_status: str
    error_message: str

class DocumentProcessingAgent:
    """åŸºäºLangGraphçš„æ–‡æ¡£å¤„ç†æ™ºèƒ½ä½“"""
    
    def __init__(self):
        self.graph = self._build_workflow()
        
    def _build_workflow(self) -> StateGraph:
        """æ„å»ºæ–‡æ¡£å¤„ç†å·¥ä½œæµ"""
        workflow = StateGraph(DocumentProcessingState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("parse_document", self.parse_document)
        workflow.add_node("extract_structure", self.extract_structure)
        workflow.add_node("semantic_chunking", self.semantic_chunking)
        workflow.add_node("generate_embeddings", self.generate_embeddings)
        workflow.add_node("extract_entities", self.extract_entities)
        workflow.add_node("build_knowledge_graph", self.build_knowledge_graph)
        workflow.add_node("store_results", self.store_results)
        
        # å®šä¹‰å·¥ä½œæµ
        workflow.set_entry_point("parse_document")
        workflow.add_edge("parse_document", "extract_structure")
        workflow.add_edge("extract_structure", "semantic_chunking")
        workflow.add_edge("semantic_chunking", "generate_embeddings")
        workflow.add_edge("generate_embeddings", "extract_entities")
        workflow.add_edge("extract_entities", "build_knowledge_graph")
        workflow.add_edge("build_knowledge_graph", "store_results")
        workflow.add_edge("store_results", END)
        
        return workflow.compile()
    
    async def parse_document(self, state: DocumentProcessingState) -> DocumentProcessingState:
        """è§£ææ–‡æ¡£"""
        from langchain_community.document_loaders import PyPDFLoader, UnstructuredWordDocumentLoader
        
        try:
            if state["document_path"].endswith('.pdf'):
                loader = PyPDFLoader(state["document_path"])
            elif state["document_path"].endswith('.docx'):
                loader = UnstructuredWordDocumentLoader(state["document_path"])
            else:
                raise ValueError(f"Unsupported document type: {state['document_path']}")
            
            documents = await loader.aload()
            raw_content = "\n".join([doc.page_content for doc in documents])
            
            state["raw_content"] = raw_content
            state["processing_status"] = "parsed"
            
        except Exception as e:
            state["error_message"] = str(e)
            state["processing_status"] = "error"
            
        return state
    
    async def semantic_chunking(self, state: DocumentProcessingState) -> DocumentProcessingState:
        """è¯­ä¹‰åˆ†å—"""
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        from langchain_experimental.text_splitter import SemanticChunker
        from langchain_openai import OpenAIEmbeddings
        
        try:
            # ä½¿ç”¨è¯­ä¹‰åˆ†å—å™¨
            embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
            text_splitter = SemanticChunker(
                embeddings=embeddings,
                breakpoint_threshold_type="percentile",
                breakpoint_threshold_amount=80
            )
            
            chunks = text_splitter.create_documents([state["raw_content"]])
            
            state["chunks"] = [
                {
                    "content": chunk.page_content,
                    "metadata": chunk.metadata,
                    "index": i
                }
                for i, chunk in enumerate(chunks)
            ]
            state["processing_status"] = "chunked"
            
        except Exception as e:
            state["error_message"] = str(e)
            state["processing_status"] = "error"
            
        return state
```

#### 2.1.2 LangChainå·¥å…·é›†æˆ
```python
from langchain_core.tools import tool
from langchain_community.vectorstores import Qdrant
from langchain_openai import OpenAIEmbeddings

@tool
def store_embeddings_tool(chunks: List[dict], project_id: str) -> dict:
    """å­˜å‚¨å‘é‡åµŒå…¥åˆ°Qdrant"""
    try:
        embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
        
        # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
        documents = [
            Document(
                page_content=chunk["content"],
                metadata={
                    **chunk["metadata"],
                    "project_id": project_id,
                    "chunk_index": chunk["index"]
                }
            )
            for chunk in chunks
        ]
        
        # å­˜å‚¨åˆ°Qdrant
        vectorstore = Qdrant.from_documents(
            documents,
            embeddings,
            url="http://localhost:6333",
            collection_name=f"project_{project_id}"
        )
        
        return {"status": "success", "count": len(chunks)}
        
    except Exception as e:
        return {"status": "error", "message": str(e)}

@tool
def extract_entities_tool(text: str) -> List[dict]:
    """æå–å‘½åå®ä½“"""
    from langchain_openai import ChatOpenAI
    from langchain_core.prompts import ChatPromptTemplate
    
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    
    prompt = ChatPromptTemplate.from_template("""
    ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–å…³é”®å®ä½“ï¼ŒåŒ…æ‹¬ï¼š
    - äººå (PERSON)
    - ç»„ç»‡æœºæ„ (ORGANIZATION) 
    - åœ°ç‚¹ (LOCATION)
    - æ—¥æœŸ (DATE)
    - ä¸“ä¸šæœ¯è¯­ (TERM)
    
    æ–‡æœ¬: {text}
    
    è¿”å›JSONæ ¼å¼:
    {{"entities": [{{"text": "å®ä½“", "type": "ç±»å‹", "start": 0, "end": 2}}]}}
    """)
    
    chain = prompt | llm
    result = chain.invoke({"text": text[:2000]})  # é™åˆ¶é•¿åº¦
    
    try:
        import json
        return json.loads(result.content)["entities"]
    except:
        return []
```

### 2.2 ç ”ç©¶è§„åˆ’æ™ºèƒ½ä½“ (Research Planning Agent)

#### 2.2.1 åŸºäºDeepResearchçš„è§„åˆ’å·¥ä½œæµ
```python
from langgraph import StateGraph, END
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

class ResearchPlanningState(TypedDict):
    """ç ”ç©¶è§„åˆ’çŠ¶æ€"""
    research_query: str
    project_id: str
    existing_documents: List[str]
    research_plan: dict
    search_queries: List[str]
    current_step: int
    total_steps: int
    findings: List[dict]
    synthesis_report: str
    status: str

class ResearchPlanningAgent:
    """åŸºäºDeepResearchå·¥ä½œæµçš„ç ”ç©¶è§„åˆ’æ™ºèƒ½ä½“"""
    
    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4o", temperature=0.1)
        self.graph = self._build_workflow()
    
    def _build_workflow(self) -> StateGraph:
        """æ„å»ºç ”ç©¶è§„åˆ’å·¥ä½œæµ"""
        workflow = StateGraph(ResearchPlanningState)
        
        # æ·»åŠ èŠ‚ç‚¹ - å‚è€ƒDeepResearchå·¥ä½œæµ
        workflow.add_node("analyze_query", self.analyze_research_query)
        workflow.add_node("create_plan", self.create_research_plan)
        workflow.add_node("generate_searches", self.generate_search_queries)
        workflow.add_node("execute_search", self.execute_search_step)
        workflow.add_node("synthesize_findings", self.synthesize_findings)
        workflow.add_node("generate_report", self.generate_final_report)
        
        # å®šä¹‰å·¥ä½œæµè·¯å¾„
        workflow.set_entry_point("analyze_query")
        workflow.add_edge("analyze_query", "create_plan")
        workflow.add_edge("create_plan", "generate_searches")
        workflow.add_edge("generate_searches", "execute_search")
        
        # æ¡ä»¶è·¯å¾„ï¼šç»§ç»­æœç´¢æˆ–è¿›å…¥ç»¼åˆé˜¶æ®µ
        workflow.add_conditional_edges(
            "execute_search",
            self._should_continue_search,
            {
                "continue": "execute_search",
                "synthesize": "synthesize_findings"
            }
        )
        
        workflow.add_edge("synthesize_findings", "generate_report")
        workflow.add_edge("generate_report", END)
        
        return workflow.compile()
    
    async def analyze_research_query(self, state: ResearchPlanningState) -> ResearchPlanningState:
        """åˆ†æç ”ç©¶æŸ¥è¯¢"""
        prompt = ChatPromptTemplate.from_template("""
        åˆ†æä»¥ä¸‹ç ”ç©¶æŸ¥è¯¢ï¼Œç¡®å®šç ”ç©¶çš„èŒƒå›´ã€æ·±åº¦å’Œå…³é”®æ–¹å‘ï¼š
        
        æŸ¥è¯¢: {query}
        ç°æœ‰æ–‡æ¡£: {documents}
        
        è¯·æä¾›ï¼š
        1. ç ”ç©¶ä¸»é¢˜çš„æ ¸å¿ƒæ¦‚å¿µ
        2. éœ€è¦æ·±å…¥æ¢ç´¢çš„å­é¢†åŸŸ
        3. å¯èƒ½çš„ç ”ç©¶è§’åº¦å’Œæ–¹æ³•
        4. é¢„æœŸçš„ç ”ç©¶æ·±åº¦å’Œå¹¿åº¦
        
        è¿”å›JSONæ ¼å¼çš„åˆ†æç»“æœã€‚
        """)
        
        chain = prompt | self.llm
        result = await chain.ainvoke({
            "query": state["research_query"],
            "documents": ", ".join(state["existing_documents"])
        })
        
        try:
            import json
            analysis = json.loads(result.content)
            state["status"] = "analyzed"
            state["research_analysis"] = analysis
        except Exception as e:
            state["status"] = "error"
            state["error"] = str(e)
            
        return state
    
    async def create_research_plan(self, state: ResearchPlanningState) -> ResearchPlanningState:
        """åˆ›å»ºè¯¦ç»†çš„ç ”ç©¶è®¡åˆ’"""
        prompt = ChatPromptTemplate.from_template("""
        åŸºäºç ”ç©¶åˆ†æï¼Œåˆ›å»ºä¸€ä¸ªè¯¦ç»†çš„åˆ†é˜¶æ®µç ”ç©¶è®¡åˆ’ï¼š
        
        ç ”ç©¶æŸ¥è¯¢: {query}
        åˆ†æç»“æœ: {analysis}
        
        è¯·åˆ›å»ºåŒ…å«ä»¥ä¸‹è¦ç´ çš„ç ”ç©¶è®¡åˆ’ï¼š
        1. ç ”ç©¶é˜¶æ®µåˆ’åˆ†ï¼ˆ3-5ä¸ªé˜¶æ®µï¼‰
        2. æ¯ä¸ªé˜¶æ®µçš„å…·ä½“ç›®æ ‡
        3. éœ€è¦æœç´¢çš„å…³é”®è¯å’Œæ¦‚å¿µ
        4. é¢„æœŸçš„è¾“å‡ºå’Œé‡Œç¨‹ç¢‘
        5. é˜¶æ®µé—´çš„ä¾èµ–å…³ç³»
        
        è¿”å›JSONæ ¼å¼çš„ç ”ç©¶è®¡åˆ’ã€‚
        """)
        
        chain = prompt | self.llm
        result = await chain.ainvoke({
            "query": state["research_query"],
            "analysis": state.get("research_analysis", {})
        })
        
        try:
            import json
            plan = json.loads(result.content)
            state["research_plan"] = plan
            state["total_steps"] = len(plan.get("stages", []))
            state["current_step"] = 0
            state["status"] = "planned"
        except Exception as e:
            state["status"] = "error"
            state["error"] = str(e)
            
        return state

### 2.3 çŸ¥è¯†é—®ç­”æ™ºèƒ½ä½“ (Knowledge QA Agent)

#### 2.3.1 RAGå¢å¼ºçš„é—®ç­”ç³»ç»Ÿ
```python
from langchain.chains import RetrievalQA
from langchain_community.vectorstores import Qdrant
from langchain_core.output_parsers import JsonOutputParser

class KnowledgeQAState(TypedDict):
    """çŸ¥è¯†é—®ç­”çŠ¶æ€"""
    question: str
    project_id: str
    context_documents: List[str]
    retrieved_chunks: List[dict]
    answer: str
    sources: List[dict]
    confidence_score: float
    follow_up_questions: List[str]
    status: str

class KnowledgeQAAgent:
    """åŸºäºRAGçš„çŸ¥è¯†é—®ç­”æ™ºèƒ½ä½“"""
    
    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4o", temperature=0)
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
        self.graph = self._build_workflow()
    
    def _build_workflow(self) -> StateGraph:
        """æ„å»ºé—®ç­”å·¥ä½œæµ"""
        workflow = StateGraph(KnowledgeQAState)
        
        workflow.add_node("analyze_question", self.analyze_question)
        workflow.add_node("retrieve_context", self.retrieve_relevant_context)
        workflow.add_node("rerank_results", self.rerank_retrieved_results)
        workflow.add_node("generate_answer", self.generate_contextual_answer)
        workflow.add_node("validate_answer", self.validate_answer_quality)
        workflow.add_node("generate_follow_ups", self.generate_follow_up_questions)
        
        workflow.set_entry_point("analyze_question")
        workflow.add_edge("analyze_question", "retrieve_context")
        workflow.add_edge("retrieve_context", "rerank_results")
        workflow.add_edge("rerank_results", "generate_answer")
        workflow.add_edge("generate_answer", "validate_answer")
        workflow.add_edge("validate_answer", "generate_follow_ups")
        workflow.add_edge("generate_follow_ups", END)
        
        return workflow.compile()
    
    async def retrieve_relevant_context(self, state: KnowledgeQAState) -> KnowledgeQAState:
        """æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡"""
        try:
            # è¿æ¥åˆ°é¡¹ç›®ç‰¹å®šçš„å‘é‡åº“
            vectorstore = Qdrant(
                client=QdrantClient(url="http://localhost:6333"),
                collection_name=f"project_{state['project_id']}",
                embeddings=self.embeddings
            )
            
            # æ··åˆæ£€ç´¢ï¼šå‘é‡ç›¸ä¼¼åº¦ + å…³é”®è¯åŒ¹é…
            retriever = vectorstore.as_retriever(
                search_type="mmr",  # æœ€å¤§è¾¹é™…ç›¸å…³æ€§
                search_kwargs={
                    "k": 10,
                    "fetch_k": 20,
                    "lambda_mult": 0.7
                }
            )
            
            # æ£€ç´¢ç›¸å…³æ–‡æ¡£å—
            retrieved_docs = await retriever.aget_relevant_documents(state["question"])
            
            state["retrieved_chunks"] = [
                {
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "score": getattr(doc, 'score', 0.0)
                }
                for doc in retrieved_docs
            ]
            
            state["status"] = "retrieved"
            
        except Exception as e:
            state["status"] = "error"
            state["error"] = str(e)
            
        return state
    
    async def generate_contextual_answer(self, state: KnowledgeQAState) -> KnowledgeQAState:
        """ç”ŸæˆåŸºäºä¸Šä¸‹æ–‡çš„ç­”æ¡ˆ"""
        # æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([
            f"æ–‡æ¡£ç‰‡æ®µ {i+1}:\n{chunk['content']}"
            for i, chunk in enumerate(state["retrieved_chunks"][:5])
        ])
        
        prompt = ChatPromptTemplate.from_template("""
        åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ã€‚è¯·ç¡®ä¿ï¼š
        1. ç­”æ¡ˆå®Œå…¨åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡
        2. æ˜ç¡®å¼•ç”¨ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µ
        3. å¦‚æœä¸Šä¸‹æ–‡ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·è¯šå®è¯´æ˜
        4. æä¾›å‡†ç¡®çš„æºæ–‡æ¡£å¼•ç”¨
        
        ä¸Šä¸‹æ–‡:
        {context}
        
        é—®é¢˜: {question}
        
        è¯·ä»¥JSONæ ¼å¼è¿”å›ç­”æ¡ˆï¼š
        {{
            "answer": "è¯¦ç»†ç­”æ¡ˆ",
            "sources": [
                {{"chunk_index": 1, "relevance": "high", "quote": "ç›¸å…³å¼•ç”¨"}}
            ],
            "confidence": 0.85,
            "limitations": "å›ç­”çš„å±€é™æ€§è¯´æ˜"
        }}
        """)
        
        chain = prompt | self.llm | JsonOutputParser()
        
        try:
            result = await chain.ainvoke({
                "context": context,
                "question": state["question"]
            })
            
            state["answer"] = result["answer"]
            state["sources"] = result["sources"]
            state["confidence_score"] = result["confidence"]
            state["status"] = "answered"
            
        except Exception as e:
            state["status"] = "error"
            state["error"] = str(e)
            
        return state

### 2.4 è®°å¿†ç®¡ç†æ™ºèƒ½ä½“ (Memory Management Agent)

#### 2.4.1 æ¸è¿›å¼å­¦ä¹ è®°å¿†ç³»ç»Ÿ
```python
class MemoryManagementState(TypedDict):
    """è®°å¿†ç®¡ç†çŠ¶æ€"""
    project_id: str
    interaction_type: str  # "qa", "feedback", "annotation"
    content: dict
    user_feedback: dict
    memory_updates: List[dict]
    knowledge_evolution: dict
    status: str

class MemoryManagementAgent:
    """é¡¹ç›®è®°å¿†ç®¡ç†æ™ºèƒ½ä½“"""
    
    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        self.graph = self._build_workflow()
    
    def _build_workflow(self) -> StateGraph:
        """æ„å»ºè®°å¿†ç®¡ç†å·¥ä½œæµ"""
        workflow = StateGraph(MemoryManagementState)
        
        workflow.add_node("analyze_interaction", self.analyze_user_interaction)
        workflow.add_node("extract_insights", self.extract_learning_insights)
        workflow.add_node("update_memories", self.update_project_memories)
        workflow.add_node("evolve_knowledge", self.evolve_knowledge_base)
        workflow.add_node("optimize_retrieval", self.optimize_retrieval_strategy)
        
        workflow.set_entry_point("analyze_interaction")
        workflow.add_edge("analyze_interaction", "extract_insights")
        workflow.add_edge("extract_insights", "update_memories")
        workflow.add_edge("update_memories", "evolve_knowledge")
        workflow.add_edge("evolve_knowledge", "optimize_retrieval")
        workflow.add_edge("optimize_retrieval", END)
        
        return workflow.compile()
    
    async def extract_learning_insights(self, state: MemoryManagementState) -> MemoryManagementState:
        """æå–å­¦ä¹ æ´å¯Ÿ"""
        prompt = ChatPromptTemplate.from_template("""
        åˆ†æç”¨æˆ·äº¤äº’ï¼Œæå–å¯å­¦ä¹ çš„æ´å¯Ÿï¼š
        
        äº¤äº’ç±»å‹: {interaction_type}
        å†…å®¹: {content}
        ç”¨æˆ·åé¦ˆ: {feedback}
        
        è¯·è¯†åˆ«ï¼š
        1. ç”¨æˆ·çš„çŸ¥è¯†åå¥½å’Œå…´è¶£ç‚¹
        2. å¸¸è§çš„é—®é¢˜æ¨¡å¼å’Œä¸»é¢˜
        3. éœ€è¦æ”¹è¿›çš„å›ç­”è´¨é‡æ–¹é¢
        4. æ–°çš„æ¦‚å¿µå…³è”å’ŒçŸ¥è¯†è¿æ¥
        5. ç”¨æˆ·çš„ç ”ç©¶æ–¹æ³•å’Œæ€ç»´æ¨¡å¼
        
        è¿”å›JSONæ ¼å¼çš„æ´å¯Ÿåˆ†æã€‚
        """)
        
        chain = prompt | self.llm | JsonOutputParser()
        
        try:
            insights = await chain.ainvoke({
                "interaction_type": state["interaction_type"],
                "content": state["content"],
                "feedback": state.get("user_feedback", {})
            })
            
            state["learning_insights"] = insights
            state["status"] = "insights_extracted"
            
        except Exception as e:
            state["status"] = "error"
            state["error"] = str(e)
            
                 return state
```

## 3. æ•°æ®æ¨¡å‹è®¾è®¡ (å·²å®ç° âœ…)

### 3.1 æ ¸å¿ƒæ•°æ®æ¨¡å‹ (åŸºäºå®é™…å®ç°)
```python
# src/models/base.py - åŸºç¡€æ¨¡å‹
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy import Column, Integer, DateTime, func
from pydantic import BaseModel, Field
from typing import Optional
from datetime import datetime

Base = declarative_base()

class BaseEntity(Base):
    """åŸºç¡€å®ä½“ç±»"""
    __abstract__ = True
    
    id = Column(Integer, primary_key=True, index=True)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())

# src/models/user.py - ç”¨æˆ·æ¨¡å‹ (å·²å®ç°)
class User(BaseEntity):
    """ç”¨æˆ·æ¨¡å‹ - å·²å®ç°å¹¶æµ‹è¯•é€šè¿‡"""
    __tablename__ = "users"
    
    username = Column(String(50), unique=True, index=True, nullable=False)
    email = Column(String(100), unique=True, index=True, nullable=False)
    hashed_password = Column(Text, nullable=False)
    is_active = Column(Boolean, default=True)
    preferences = Column(JSON, default=dict)

class UserCreate(BaseModel):
    """ç”¨æˆ·åˆ›å»ºæ¨¡å‹"""
    username: str = Field(..., min_length=3, max_length=50)
    email: str = Field(..., regex=r'^[^@]+@[^@]+\.[^@]+$')
    password: str = Field(..., min_length=8)

class UserResponse(BaseModel):
    """ç”¨æˆ·å“åº”æ¨¡å‹"""
    id: int
    username: str
    email: str
    is_active: bool
    preferences: dict
    created_at: datetime
    updated_at: Optional[datetime] = None

# src/models/project.py - é¡¹ç›®æ¨¡å‹ (å·²å®ç°)
from enum import Enum

class ProjectType(str, Enum):
    """é¡¹ç›®ç±»å‹æšä¸¾"""
    RESEARCH = "research"
    LITERATURE_REVIEW = "literature_review"
    TECHNICAL_ANALYSIS = "technical_analysis"
    LEGAL_REVIEW = "legal_review"
    GENERAL = "general"

class Project(BaseEntity):
    """é¡¹ç›®æ¨¡å‹ - å·²å®ç°å¹¶æµ‹è¯•é€šè¿‡"""
    __tablename__ = "projects"
    
    name = Column(String(200), nullable=False, index=True)
    description = Column(Text)
    project_type = Column(Enum(ProjectType), default=ProjectType.GENERAL)
    status = Column(String(20), default="active", index=True)
    owner_id = Column(Integer, ForeignKey("users.id"), nullable=False)
    settings = Column(JSON, default=dict)
    research_query = Column(Text)
    research_plan = Column(JSON)
    
    # å…³ç³»
    owner = relationship("User", backref="projects")
    documents = relationship("Document", back_populates="project", cascade="all, delete-orphan")

# src/models/document.py - æ–‡æ¡£æ¨¡å‹ (å·²å®ç°)
class DocumentType(str, Enum):
    """æ–‡æ¡£ç±»å‹æšä¸¾"""
    PDF = "pdf"
    DOCX = "docx"
    MARKDOWN = "markdown"
    TXT = "txt"
    HTML = "html"

class ProcessingStatus(str, Enum):
    """å¤„ç†çŠ¶æ€æšä¸¾"""
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"

class Document(BaseEntity):
    """æ–‡æ¡£æ¨¡å‹ - å·²å®ç°å¹¶æµ‹è¯•é€šè¿‡"""
    __tablename__ = "documents"
    
    title = Column(String(500), nullable=False, index=True)
    filename = Column(String(255), nullable=False)
    file_path = Column(String(1000), nullable=False)
    file_hash = Column(String(64), unique=True, index=True)
    file_size = Column(BigInteger)
    document_type = Column(Enum(DocumentType), nullable=False)
    processing_status = Column(Enum(ProcessingStatus), default=ProcessingStatus.PENDING)
    content = Column(Text)
    summary = Column(Text)
    metadata = Column(JSON, default=dict)
    project_id = Column(Integer, ForeignKey("projects.id"), nullable=False)
    
    # å…³ç³»
    project = relationship("Project", back_populates="documents")
    chunks = relationship("Chunk", back_populates="document", cascade="all, delete-orphan")

# src/models/chunk.py - æ–‡æ¡£å—æ¨¡å‹ (å·²å®ç°)
class ChunkType(str, Enum):
    """å—ç±»å‹æšä¸¾"""
    TEXT = "text"
    TABLE = "table"
    IMAGE = "image"
    CODE = "code"
    FORMULA = "formula"

class Chunk(BaseEntity):
    """æ–‡æ¡£å—æ¨¡å‹ - å·²å®ç°å¹¶æµ‹è¯•é€šè¿‡"""
    __tablename__ = "chunks"
    
    document_id = Column(Integer, ForeignKey("documents.id"), nullable=False, index=True)
    chunk_index = Column(Integer, nullable=False)
    content = Column(Text, nullable=False)
    content_hash = Column(String(64), index=True)
    vector_id = Column(String(100), index=True)  # Qdrantå‘é‡ID
    
    # ä½ç½®ä¿¡æ¯
    start_page = Column(Integer)
    end_page = Column(Integer)
    start_char = Column(Integer)
    end_char = Column(Integer)
    
    # ç±»å‹å’Œå…ƒæ•°æ®
    chunk_type = Column(Enum(ChunkType), default=ChunkType.TEXT)
    token_count = Column(Integer)
    metadata = Column(JSON, default=dict)
    
    # å…³ç³»
    document = relationship("Document", back_populates="chunks")

# src/models/conversation.py - å¯¹è¯æ¨¡å‹ (å·²å®ç°)
class MessageRole(str, Enum):
    """æ¶ˆæ¯è§’è‰²æšä¸¾"""
    USER = "user"
    ASSISTANT = "assistant"
    SYSTEM = "system"

class Conversation(BaseEntity):
    """å¯¹è¯æ¨¡å‹ - å·²å®ç°å¹¶æµ‹è¯•é€šè¿‡"""
    __tablename__ = "conversations"
    
    project_id = Column(Integer, ForeignKey("projects.id"), nullable=False, index=True)
    user_id = Column(Integer, ForeignKey("users.id"), nullable=False, index=True)
    title = Column(String(500))
    summary = Column(Text)
    message_metadata = Column(JSON, default=list)  # é¿å…ä¸SQLAlchemy metadataå†²çª
    status = Column(String(20), default="active")
    
    # å…³ç³»
    project = relationship("Project")
    user = relationship("User")

class ConversationMessage(BaseModel):
    """å¯¹è¯æ¶ˆæ¯æ¨¡å‹"""
    role: MessageRole
    content: str
    timestamp: datetime
    metadata: dict = Field(default_factory=dict)

class ConversationCreate(BaseModel):
    """å¯¹è¯åˆ›å»ºæ¨¡å‹"""
    project_id: int
    title: Optional[str] = None
    initial_message: Optional[str] = None

### 3.2 LangGraphçŠ¶æ€æ¨¡å‹
```python
class GlobalAgentState(TypedDict):
    """å…¨å±€æ™ºèƒ½ä½“çŠ¶æ€"""
    project_id: str
    user_id: str
    session_id: str
    current_task: str
    task_history: List[dict]
    context_memory: dict
    error_state: Optional[dict]
    
class DocumentProcessingResult(BaseModel):
    """æ–‡æ¡£å¤„ç†ç»“æœ"""
    document_id: int
    chunks_created: int
    entities_extracted: int
    embeddings_stored: bool
    knowledge_graph_updated: bool
    processing_time: float
    status: ProcessingStatus
    error_message: Optional[str] = None

class ResearchPlanResult(BaseModel):
    """ç ”ç©¶è§„åˆ’ç»“æœ"""
    plan_id: str
    stages: List[dict]
    total_estimated_time: int
    key_concepts: List[str]
    search_strategies: List[str]
    success_criteria: List[str]

class QAResult(BaseModel):
    """é—®ç­”ç»“æœ"""
    question: str
    answer: str
    sources: List[dict]
    confidence_score: float
    response_time: float
    follow_up_questions: List[str]
    limitations: Optional[str] = None

## 4. APIè®¾è®¡

### 4.1 RESTful APIç«¯ç‚¹
```python
from fastapi import FastAPI, Depends, HTTPException, UploadFile, File
from fastapi.security import HTTPBearer
from typing import List, Optional

app = FastAPI(title="DPAæ™ºèƒ½çŸ¥è¯†å¼•æ“", version="3.0.0")
security = HTTPBearer()

# é¡¹ç›®ç®¡ç†API
@app.post("/api/v1/projects", response_model=Project)
async def create_project(
    project: Project,
    current_user: User = Depends(get_current_user)
):
    """åˆ›å»ºæ–°é¡¹ç›®"""
    pass

@app.get("/api/v1/projects", response_model=List[Project])
async def list_projects(
    current_user: User = Depends(get_current_user),
    skip: int = 0,
    limit: int = 10
):
    """è·å–é¡¹ç›®åˆ—è¡¨"""
    pass

@app.get("/api/v1/projects/{project_id}", response_model=Project)
async def get_project(
    project_id: int,
    current_user: User = Depends(get_current_user)
):
    """è·å–é¡¹ç›®è¯¦æƒ…"""
    pass

# æ–‡æ¡£ç®¡ç†API
@app.post("/api/v1/projects/{project_id}/documents")
async def upload_document(
    project_id: int,
    file: UploadFile = File(...),
    document_type: DocumentType = DocumentType.ACADEMIC_PAPER,
    current_user: User = Depends(get_current_user)
):
    """ä¸Šä¼ æ–‡æ¡£"""
    # å¯åŠ¨æ–‡æ¡£å¤„ç†æ™ºèƒ½ä½“
    agent = DocumentProcessingAgent()
    result = await agent.graph.ainvoke({
        "project_id": str(project_id),
        "document_path": file.filename,
        "document_type": document_type.value,
        "processing_status": "pending"
    })
    return result

@app.get("/api/v1/projects/{project_id}/documents", response_model=List[Document])
async def list_documents(
    project_id: int,
    current_user: User = Depends(get_current_user)
):
    """è·å–æ–‡æ¡£åˆ—è¡¨"""
    pass

# çŸ¥è¯†é—®ç­”API
@app.post("/api/v1/projects/{project_id}/qa")
async def ask_question(
    project_id: int,
    question: str,
    context_documents: Optional[List[int]] = None,
    current_user: User = Depends(get_current_user)
):
    """çŸ¥è¯†é—®ç­”"""
    agent = KnowledgeQAAgent()
    result = await agent.graph.ainvoke({
        "question": question,
        "project_id": str(project_id),
        "context_documents": context_documents or [],
        "status": "pending"
    })
    return result

# ç ”ç©¶è§„åˆ’API
@app.post("/api/v1/projects/{project_id}/research-plan")
async def create_research_plan(
    project_id: int,
    research_query: str,
    current_user: User = Depends(get_current_user)
):
    """åˆ›å»ºç ”ç©¶è§„åˆ’"""
    agent = ResearchPlanningAgent()
    result = await agent.graph.ainvoke({
        "research_query": research_query,
        "project_id": str(project_id),
        "existing_documents": [],
        "status": "pending"
    })
    return result

@app.get("/api/v1/projects/{project_id}/research-plan")
async def get_research_plan(
    project_id: int,
    current_user: User = Depends(get_current_user)
):
    """è·å–ç ”ç©¶è§„åˆ’"""
    pass

# è®°å¿†ç®¡ç†API
@app.post("/api/v1/projects/{project_id}/memory/feedback")
async def submit_feedback(
    project_id: int,
    interaction_id: str,
    feedback: dict,
    current_user: User = Depends(get_current_user)
):
    """æäº¤ç”¨æˆ·åé¦ˆ"""
    agent = MemoryManagementAgent()
    result = await agent.graph.ainvoke({
        "project_id": str(project_id),
        "interaction_type": "feedback",
        "content": {"interaction_id": interaction_id},
        "user_feedback": feedback,
        "status": "pending"
    })
    return result

### 4.2 WebSocketå®æ—¶é€šä¿¡
```python
from fastapi import WebSocket, WebSocketDisconnect
import json

class ConnectionManager:
    """WebSocketè¿æ¥ç®¡ç†å™¨"""
    
    def __init__(self):
        self.active_connections: Dict[str, WebSocket] = {}
    
    async def connect(self, websocket: WebSocket, client_id: str):
        await websocket.accept()
        self.active_connections[client_id] = websocket
    
    def disconnect(self, client_id: str):
        if client_id in self.active_connections:
            del self.active_connections[client_id]
    
    async def send_personal_message(self, message: dict, client_id: str):
        if client_id in self.active_connections:
            await self.active_connections[client_id].send_text(json.dumps(message))

manager = ConnectionManager()

@app.websocket("/ws/{project_id}/{user_id}")
async def websocket_endpoint(websocket: WebSocket, project_id: int, user_id: int):
    """WebSocketç«¯ç‚¹ç”¨äºå®æ—¶é€šä¿¡"""
    client_id = f"{project_id}_{user_id}"
    await manager.connect(websocket, client_id)
    
    try:
        while True:
            data = await websocket.receive_text()
            message = json.loads(data)
            
            # å¤„ç†ä¸åŒç±»å‹çš„æ¶ˆæ¯
            if message["type"] == "document_processing_status":
                # å‘é€æ–‡æ¡£å¤„ç†çŠ¶æ€æ›´æ–°
                await manager.send_personal_message({
                    "type": "processing_update",
                    "status": "processing",
                    "progress": 50
                }, client_id)
            
            elif message["type"] == "qa_request":
                # å¤„ç†å®æ—¶é—®ç­”è¯·æ±‚
                agent = KnowledgeQAAgent()
                result = await agent.graph.ainvoke({
                    "question": message["question"],
                    "project_id": str(project_id),
                    "status": "pending"
                })
                
                await manager.send_personal_message({
                    "type": "qa_response",
                    "result": result
                }, client_id)
                
         except WebSocketDisconnect:
         manager.disconnect(client_id)
```

## 5. éƒ¨ç½²æ¶æ„ä¸é…ç½® (åŸºäºå®é™…ç¯å¢ƒ âœ…)

### 5.1 å¼€å‘ç¯å¢ƒé…ç½® (dpa_gen condaç¯å¢ƒ)
```bash
# ç¯å¢ƒæ¿€æ´»
conda activate dpa_gen

# æ ¸å¿ƒä¾èµ–ç‰ˆæœ¬ (å·²å®‰è£…å¹¶éªŒè¯)
Python: 3.11.5
FastAPI: 0.115.13
LangChain: 0.3.26
LangGraph: 0.4.8
LangSmith: 0.4.1
Pydantic: 2.10.3
SQLAlchemy: 2.0.41
Alembic: 1.16.2

# æ•°æ®åº“è¿æ¥ (rtx4080æœåŠ¡å™¨)
PostgreSQL: rtx4080:5432 (dpa_dev, dpa_test, dpa_prod)
Qdrant: rtx4080:6333 (å‘é‡æ•°æ®åº“)
Neo4j: rtx4080:7687 (çŸ¥è¯†å›¾è°±)
Redis: rtx4080:6379 (ç¼“å­˜å’Œä¼šè¯)
```

### 5.2 ç”Ÿäº§ç¯å¢ƒé…ç½® (.envæ–‡ä»¶)
```bash
# æ•°æ®åº“é…ç½® (å·²é…ç½®)
DATABASE_URL=postgresql://dpa_user:dpa_password@rtx4080:5432/dpa_prod
DATABASE_DEV_URL=postgresql://dpa_user:dpa_password@rtx4080:5432/dpa_dev
DATABASE_TEST_URL=postgresql://dpa_user:dpa_password@rtx4080:5432/dpa_test

# å‘é‡æ•°æ®åº“
QDRANT_URL=http://rtx4080:6333
QDRANT_API_KEY=your_qdrant_api_key

# å›¾æ•°æ®åº“
NEO4J_URL=bolt://rtx4080:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=your_neo4j_password

# ç¼“å­˜
REDIS_URL=redis://rtx4080:6379
REDIS_PASSWORD=your_redis_password

# AIæ¨¡å‹API (å·²é…ç½®)
OPENAI_API_KEY=your_openai_api_key
ANTHROPIC_API_KEY=your_anthropic_api_key
COHERE_API_KEY=your_cohere_api_key
OPENROUTER_API_KEY=your_openrouter_api_key

# LangSmithç›‘æ§
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your_langsmith_api_key
LANGCHAIN_PROJECT=dpa-development

# åº”ç”¨é…ç½®
APP_NAME=DPAæ™ºèƒ½çŸ¥è¯†å¼•æ“
APP_VERSION=4.0.0
DEBUG=false
SECRET_KEY=your_secret_key
ALLOWED_HOSTS=localhost,127.0.0.1,rtx4080

# æ–‡ä»¶å­˜å‚¨
UPLOAD_DIR=./uploads
DATA_DIR=./data
CACHE_DIR=./data/cache
LOGS_DIR=./data/logs
```

### 5.3 Docker Composeé…ç½® (é€‚é…rtx4080ç¯å¢ƒ)
```yaml
# docker-compose.dev.yml
version: '3.8'

services:
  # åç«¯APIæœåŠ¡
  api:
    build:
      context: .
      dockerfile: backend/Dockerfile.dev
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://dpa_user:dpa_password@rtx4080:5432/dpa_dev
      - QDRANT_URL=http://rtx4080:6333
      - NEO4J_URL=bolt://rtx4080:7687
      - REDIS_URL=redis://rtx4080:6379
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - LANGCHAIN_API_KEY=${LANGCHAIN_API_KEY}
    volumes:
      - ./src:/app/src
      - ./data:/app/data
      - ./uploads:/app/uploads
      - ./logs:/app/logs
    depends_on:
      - db-setup
    networks:
      - dpa-network
    
  # æ•°æ®åº“åˆå§‹åŒ–æœåŠ¡
  db-setup:
    build:
      context: .
      dockerfile: backend/Dockerfile.dev
    environment:
      - DATABASE_URL=postgresql://dpa_user:dpa_password@rtx4080:5432/dpa_dev
    volumes:
      - ./scripts:/app/scripts
    command: python scripts/setup_databases.py
    networks:
      - dpa-network
    
  # å‰ç«¯æœåŠ¡ (æœªæ¥)
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.dev
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_API_URL=http://localhost:8000
      - NEXT_PUBLIC_WS_URL=ws://localhost:8000
    volumes:
      - ./frontend:/app
      - /app/node_modules
    networks:
      - dpa-network

networks:
  dpa-network:
    driver: bridge

# æ³¨æ„ï¼šæ•°æ®åº“æœåŠ¡è¿è¡Œåœ¨rtx4080æœåŠ¡å™¨ä¸Šï¼Œä¸åœ¨Dockerä¸­
```

### 5.4 condaç¯å¢ƒé…ç½®æ–‡ä»¶
```yaml
# environment.yml (å·²ç”Ÿæˆ)
name: dpa_gen
channels:
  - conda-forge
  - defaults
dependencies:
  - python=3.11.5
  - pip
  - pip:
    - fastapi==0.115.13
    - uvicorn[standard]==0.34.3
    - langchain==0.3.26
    - langchain-community==0.3.26
    - langchain-core==0.3.66
    - langchain-openai==0.3.24
    - langgraph==0.4.8
    - langsmith==0.4.1
    - pydantic==2.10.3
    - sqlalchemy==2.0.41
    - alembic==1.16.2
    - psycopg2-binary==2.9.10
    - qdrant-client==1.14.3
    - neo4j==5.28.1
    - redis==5.2.1
    - openai==1.90.0
    - anthropic==0.43.1
    - cohere==5.16.0
    - pypdf==5.6.1
    - python-docx==1.2.0
    - markdown==3.8.2
    - beautifulsoup4==4.13.4
    - lxml==5.4.0
    - numpy==2.0.1
    - pandas==2.2.3
    - scikit-learn==1.7.0
    - aiofiles==24.1.0
    - aiohttp==3.12.13
    - requests==2.32.4
    - structlog==25.4.0
    - pytest==8.3.4
    - pytest-asyncio==0.25.0
    - pytest-cov==6.0.0
    - ruff==0.8.9
    - mypy==1.13.0
    - bandit==1.8.0
```

### 5.5 åº”ç”¨å¯åŠ¨è„šæœ¬
```bash
#!/bin/bash
# scripts/dev_setup.sh (å·²å­˜åœ¨)

# æ¿€æ´»condaç¯å¢ƒ
conda activate dpa_gen

# æ£€æŸ¥ç¯å¢ƒå˜é‡
if [ ! -f ".env" ]; then
    echo "é”™è¯¯: .envæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆé…ç½®ç¯å¢ƒå˜é‡"
    exit 1
fi

# æ£€æŸ¥æ•°æ®åº“è¿æ¥
echo "æ£€æŸ¥æ•°æ®åº“è¿æ¥..."
python scripts/test_config.py

# è¿è¡Œæ•°æ®åº“è¿ç§»
echo "è¿è¡Œæ•°æ®åº“è¿ç§»..."
alembic upgrade head

# å¯åŠ¨å¼€å‘æœåŠ¡å™¨
echo "å¯åŠ¨å¼€å‘æœåŠ¡å™¨..."
uvicorn src.api.main:app --host 0.0.0.0 --port 8000 --reload
```

### 5.6 ç³»ç»Ÿç›‘æ§é…ç½®
```python
# src/utils/monitoring.py
import structlog
from langsmith import Client as LangSmithClient
from typing import Dict, Any
import time

# ç»“æ„åŒ–æ—¥å¿—é…ç½®
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    wrapper_class=structlog.stdlib.BoundLogger,
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()

# LangSmithç›‘æ§
langsmith_client = LangSmithClient()

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    @staticmethod
    def track_agent_execution(agent_name: str, input_data: Dict[str, Any]):
        """è·Ÿè¸ªæ™ºèƒ½ä½“æ‰§è¡Œæ€§èƒ½"""
        def decorator(func):
            async def wrapper(*args, **kwargs):
                start_time = time.time()
                try:
                    result = await func(*args, **kwargs)
                    duration = time.time() - start_time
                    
                    logger.info(
                        "agent_execution_success",
                        agent_name=agent_name,
                        duration=duration,
                        input_size=len(str(input_data)),
                        output_size=len(str(result))
                    )
                    return result
                    
                except Exception as e:
                    duration = time.time() - start_time
                    logger.error(
                        "agent_execution_error",
                        agent_name=agent_name,
                        duration=duration,
                        error=str(e)
                    )
                    raise
            return wrapper
        return decorator
```
```

### 5.2 LangGraphå·¥ä½œæµç¼–æ’
```python
# src/graphs/master_workflow.py
from langgraph import StateGraph, END
from typing import TypedDict, List, Dict, Any

class MasterWorkflowState(TypedDict):
    """ä¸»å·¥ä½œæµçŠ¶æ€"""
    project_id: str
    user_id: str
    task_type: str  # "document_upload", "research_plan", "qa", "memory_update"
    input_data: Dict[str, Any]
    current_agent: str
    agent_results: Dict[str, Any]
    final_result: Dict[str, Any]
    error_state: Dict[str, Any]
    status: str

class MasterWorkflow:
    """ä¸»å·¥ä½œæµç¼–æ’å™¨"""
    
    def __init__(self):
        self.document_agent = DocumentProcessingAgent()
        self.research_agent = ResearchPlanningAgent()
        self.qa_agent = KnowledgeQAAgent()
        self.memory_agent = MemoryManagementAgent()
        self.graph = self._build_master_workflow()
    
    def _build_master_workflow(self) -> StateGraph:
        """æ„å»ºä¸»å·¥ä½œæµ"""
        workflow = StateGraph(MasterWorkflowState)
        
        # æ·»åŠ è·¯ç”±èŠ‚ç‚¹
        workflow.add_node("route_task", self.route_to_agent)
        workflow.add_node("document_processing", self.handle_document_processing)
        workflow.add_node("research_planning", self.handle_research_planning)
        workflow.add_node("knowledge_qa", self.handle_knowledge_qa)
        workflow.add_node("memory_management", self.handle_memory_management)
        workflow.add_node("aggregate_results", self.aggregate_results)
        workflow.add_node("error_handling", self.handle_errors)
        
        # è®¾ç½®å·¥ä½œæµè·¯å¾„
        workflow.set_entry_point("route_task")
        
        # æ¡ä»¶è·¯ç”±
        workflow.add_conditional_edges(
            "route_task",
            self._determine_agent,
            {
                "document": "document_processing",
                "research": "research_planning", 
                "qa": "knowledge_qa",
                "memory": "memory_management",
                "error": "error_handling"
            }
        )
        
        # æ‰€æœ‰æ™ºèƒ½ä½“å®Œæˆåèšåˆç»“æœ
        workflow.add_edge("document_processing", "aggregate_results")
        workflow.add_edge("research_planning", "aggregate_results")
        workflow.add_edge("knowledge_qa", "aggregate_results")
        workflow.add_edge("memory_management", "aggregate_results")
        workflow.add_edge("error_handling", "aggregate_results")
        workflow.add_edge("aggregate_results", END)
        
        return workflow.compile()
    
    def _determine_agent(self, state: MasterWorkflowState) -> str:
        """ç¡®å®šä½¿ç”¨å“ªä¸ªæ™ºèƒ½ä½“"""
        task_type = state["task_type"]
        
        if task_type == "document_upload":
            return "document"
        elif task_type == "research_plan":
            return "research"
        elif task_type == "qa":
            return "qa"
        elif task_type == "memory_update":
            return "memory"
        else:
            return "error"
    
    async def handle_document_processing(self, state: MasterWorkflowState) -> MasterWorkflowState:
        """å¤„ç†æ–‡æ¡£å¤„ç†ä»»åŠ¡"""
        try:
            result = await self.document_agent.graph.ainvoke({
                "project_id": state["project_id"],
                "document_path": state["input_data"]["file_path"],
                "document_type": state["input_data"]["document_type"],
                "processing_status": "pending"
            })
            
            state["agent_results"]["document"] = result
            state["current_agent"] = "document"
            state["status"] = "completed"
            
        except Exception as e:
            state["error_state"] = {"agent": "document", "error": str(e)}
            state["status"] = "error"
            
        return state

### 5.3 ç›‘æ§å’Œæ—¥å¿—ç³»ç»Ÿ
```python
# src/utils/monitoring.py
import logging
import time
from functools import wraps
from typing import Dict, Any
from langsmith import traceable
from prometheus_client import Counter, Histogram, Gauge
import structlog

# PrometheusæŒ‡æ ‡
REQUEST_COUNT = Counter('dpa_requests_total', 'Total requests', ['method', 'endpoint', 'status'])
REQUEST_DURATION = Histogram('dpa_request_duration_seconds', 'Request duration')
ACTIVE_AGENTS = Gauge('dpa_active_agents', 'Number of active agents')
DOCUMENT_PROCESSING_TIME = Histogram('dpa_document_processing_seconds', 'Document processing time')

# ç»“æ„åŒ–æ—¥å¿—é…ç½®
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()

def monitor_agent_performance(agent_name: str):
    """æ™ºèƒ½ä½“æ€§èƒ½ç›‘æ§è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        @traceable(name=f"{agent_name}_execution")
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            ACTIVE_AGENTS.inc()
            
            try:
                logger.info(
                    "agent_execution_started",
                    agent=agent_name,
                    function=func.__name__
                )
                
                result = await func(*args, **kwargs)
                
                duration = time.time() - start_time
                REQUEST_DURATION.observe(duration)
                
                logger.info(
                    "agent_execution_completed",
                    agent=agent_name,
                    function=func.__name__,
                    duration=duration,
                    status="success"
                )
                
                return result
                
            except Exception as e:
                duration = time.time() - start_time
                
                logger.error(
                    "agent_execution_failed",
                    agent=agent_name,
                    function=func.__name__,
                    duration=duration,
                    error=str(e),
                    status="error"
                )
                
                raise
                
            finally:
                ACTIVE_AGENTS.dec()
                
        return wrapper
    return decorator

class LangGraphTracer:
    """LangGraphæ‰§è¡Œè¿½è¸ªå™¨"""
    
    def __init__(self):
        self.execution_logs = []
    
    def trace_node_execution(self, node_name: str, state: Dict[str, Any], result: Dict[str, Any]):
        """è¿½è¸ªèŠ‚ç‚¹æ‰§è¡Œ"""
        log_entry = {
            "timestamp": time.time(),
            "node": node_name,
            "input_state": state,
            "output_state": result,
            "execution_time": time.time()
        }
        
        self.execution_logs.append(log_entry)
        
        logger.info(
            "langgraph_node_executed",
            node=node_name,
            state_keys=list(state.keys()),
            result_keys=list(result.keys())
        )

## 6. å®æ–½è®¡åˆ’

### 6.1 å¼€å‘é‡Œç¨‹ç¢‘ï¼ˆ9ä¸ªæœˆè®¡åˆ’ï¼‰

#### é˜¶æ®µ1ï¼šåŸºç¡€æ¶æ„æ­å»ºï¼ˆæœˆ1-2ï¼‰
**ç›®æ ‡**: å»ºç«‹åŸºäºLangGraph/LangChainçš„æ ¸å¿ƒæ¶æ„

**å…³é”®ä»»åŠ¡**:
- [ ] æ­å»ºDockerå¼€å‘ç¯å¢ƒ
- [ ] å®ç°åŸºç¡€æ•°æ®æ¨¡å‹å’Œæ•°æ®åº“è¿æ¥
- [ ] åˆ›å»ºæ–‡æ¡£å¤„ç†æ™ºèƒ½ä½“åŸºç¡€æ¡†æ¶
- [ ] é›†æˆOpenAI/Anthropic API
- [ ] å»ºç«‹Qdrantå‘é‡æ•°æ®åº“è¿æ¥
- [ ] å®ç°åŸºç¡€çš„æ–‡æ¡£ä¸Šä¼ å’Œè§£æåŠŸèƒ½

**éªŒæ”¶æ ‡å‡†**:
- èƒ½å¤ŸæˆåŠŸä¸Šä¼ PDFæ–‡æ¡£å¹¶æå–æ–‡æœ¬
- æ–‡æ¡£å†…å®¹èƒ½å¤Ÿå­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
- åŸºç¡€çš„LangGraphå·¥ä½œæµèƒ½å¤Ÿæ­£å¸¸è¿è¡Œ

#### é˜¶æ®µ2ï¼šæ ¸å¿ƒæ™ºèƒ½ä½“å¼€å‘ï¼ˆæœˆ3-4ï¼‰
**ç›®æ ‡**: å®Œæˆå››å¤§æ ¸å¿ƒæ™ºèƒ½ä½“çš„åŸºç¡€åŠŸèƒ½

**å…³é”®ä»»åŠ¡**:
- [ ] å®Œå–„æ–‡æ¡£å¤„ç†æ™ºèƒ½ä½“ï¼ˆè¯­ä¹‰åˆ†å—ã€å®ä½“æå–ï¼‰
- [ ] å®ç°çŸ¥è¯†é—®ç­”æ™ºèƒ½ä½“ï¼ˆRAGæ£€ç´¢ã€ç­”æ¡ˆç”Ÿæˆï¼‰
- [ ] å¼€å‘ç ”ç©¶è§„åˆ’æ™ºèƒ½ä½“ï¼ˆåŸºäºDeepResearchå·¥ä½œæµï¼‰
- [ ] åˆ›å»ºè®°å¿†ç®¡ç†æ™ºèƒ½ä½“ï¼ˆç”¨æˆ·åé¦ˆå­¦ä¹ ï¼‰
- [ ] å»ºç«‹æ™ºèƒ½ä½“é—´çš„åè°ƒæœºåˆ¶

**éªŒæ”¶æ ‡å‡†**:
- èƒ½å¤Ÿå¯¹å•ç¯‡æ–‡æ¡£è¿›è¡Œç²¾å‡†é—®ç­”ï¼ˆæº¯æºç‡>95%ï¼‰
- èƒ½å¤Ÿç”ŸæˆåŸºç¡€çš„ç ”ç©¶è§„åˆ’
- ç”¨æˆ·åé¦ˆèƒ½å¤Ÿè¢«æ­£ç¡®è®°å½•å’Œå­¦ä¹ 

#### é˜¶æ®µ3ï¼šå¤šæ–‡æ¡£çŸ¥è¯†æ•´åˆï¼ˆæœˆ5-6ï¼‰
**ç›®æ ‡**: å®ç°è·¨æ–‡æ¡£çš„çŸ¥è¯†æ•´åˆå’Œå…³è”åˆ†æ

**å…³é”®ä»»åŠ¡**:
- [ ] å®ç°è·¨æ–‡æ¡£æ¦‚å¿µå…³è”
- [ ] å»ºç«‹Neo4jçŸ¥è¯†å›¾è°±å­˜å‚¨
- [ ] å¼€å‘æ¦‚å¿µæ¼”åŒ–è¿½è¸ª
- [ ] å®ç°å¤šæ–‡æ¡£å¯¹æ¯”åˆ†æ
- [ ] ä¼˜åŒ–æ£€ç´¢ç­–ç•¥ï¼ˆæ··åˆæ£€ç´¢ã€é‡æ’åºï¼‰

**éªŒæ”¶æ ‡å‡†**:
- èƒ½å¤Ÿå›ç­”éœ€è¦æ•´åˆå¤šä¸ªæ–‡æ¡£ä¿¡æ¯çš„å¤æ‚é—®é¢˜
- çŸ¥è¯†å›¾è°±èƒ½å¤Ÿå¯è§†åŒ–æ¦‚å¿µå…³ç³»
- æ£€ç´¢å‡†ç¡®ç‡è¾¾åˆ°85%ä»¥ä¸Š

#### é˜¶æ®µ4ï¼šå‰ç«¯ç•Œé¢å’Œç”¨æˆ·ä½“éªŒï¼ˆæœˆ7-8ï¼‰
**ç›®æ ‡**: å®Œæˆç”¨æˆ·å‹å¥½çš„Webç•Œé¢

**å…³é”®ä»»åŠ¡**:
- [ ] å¼€å‘Next.jså‰ç«¯åº”ç”¨
- [ ] å®ç°é¡¹ç›®ç®¡ç†ç•Œé¢
- [ ] åˆ›å»ºæ–‡æ¡£ä¸Šä¼ å’Œç®¡ç†ç•Œé¢
- [ ] å¼€å‘çŸ¥è¯†é—®ç­”å¯¹è¯ç•Œé¢
- [ ] å®ç°çŸ¥è¯†å›¾è°±å¯è§†åŒ–
- [ ] å»ºç«‹WebSocketå®æ—¶é€šä¿¡

**éªŒæ”¶æ ‡å‡†**:
- ç•Œé¢å“åº”é€Ÿåº¦<2ç§’
- æ”¯æŒå®æ—¶æ–‡æ¡£å¤„ç†çŠ¶æ€æ›´æ–°
- çŸ¥è¯†å›¾è°±å¯è§†åŒ–æ¸…æ™°æ˜“æ‡‚

#### é˜¶æ®µ5ï¼šç³»ç»Ÿä¼˜åŒ–å’Œæµ‹è¯•ï¼ˆæœˆ9ï¼‰
**ç›®æ ‡**: ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–å’Œå…¨é¢æµ‹è¯•

**å…³é”®ä»»åŠ¡**:
- [ ] æ€§èƒ½ä¼˜åŒ–ï¼ˆç¼“å­˜ã€å¼‚æ­¥å¤„ç†ï¼‰
- [ ] å…¨é¢çš„å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•
- [ ] ç”¨æˆ·éªŒæ”¶æµ‹è¯•
- [ ] å®‰å…¨æ€§æµ‹è¯•å’ŒåŠ å›º
- [ ] éƒ¨ç½²è„šæœ¬å’Œæ–‡æ¡£å®Œå–„

**éªŒæ”¶æ ‡å‡†**:
- æµ‹è¯•è¦†ç›–ç‡>85%
- ç³»ç»Ÿå“åº”æ—¶é—´æ»¡è¶³PRDè¦æ±‚
- é€šè¿‡å®‰å…¨æ€§è¯„ä¼°

### 6.2 é£é™©ç¼“è§£ç­–ç•¥

**æŠ€æœ¯é£é™©**:
- **APIç¨³å®šæ€§**: é›†æˆå¤šä¸ªLLMæä¾›å•†ï¼Œå®ç°è‡ªåŠ¨æ•…éšœè½¬ç§»
- **å‘é‡æ£€ç´¢æ€§èƒ½**: ä½¿ç”¨Qdrantçš„é«˜æ€§èƒ½ç‰¹æ€§ï¼Œå®ç°åˆ†ç‰‡å’Œç¼“å­˜
- **çŸ¥è¯†å›¾è°±å¤æ‚åº¦**: æ¸è¿›å¼æ„å»ºï¼Œä»ç®€å•å…³ç³»å¼€å§‹

**èµ„æºé£é™©**:
- **APIæˆæœ¬æ§åˆ¶**: å®ç°æ™ºèƒ½ç¼“å­˜å’Œæ‰¹å¤„ç†ï¼Œç›‘æ§ä½¿ç”¨é‡
- **å¼€å‘è¿›åº¦**: é‡‡ç”¨æ•æ·å¼€å‘ï¼Œæ¯ä¸¤å‘¨ä¸€ä¸ªè¿­ä»£

**è´¨é‡é£é™©**:
- **ç­”æ¡ˆå‡†ç¡®æ€§**: å»ºç«‹è¯„ä¼°æ•°æ®é›†ï¼ŒæŒç»­ç›‘æ§å‡†ç¡®ç‡
- **ç”¨æˆ·ä½“éªŒ**: æ—©æœŸç”¨æˆ·æµ‹è¯•ï¼Œå¿«é€Ÿè¿­ä»£æ”¹è¿›

### 6.3 æˆåŠŸæŒ‡æ ‡

**æŠ€æœ¯æŒ‡æ ‡**:
- æ–‡æ¡£å¤„ç†æˆåŠŸç‡ > 98%
- é—®ç­”å“åº”æ—¶é—´ < 3ç§’
- æº¯æºå‡†ç¡®ç‡ > 95%
- ç³»ç»Ÿå¯ç”¨æ€§ > 99.5%

**ç”¨æˆ·æŒ‡æ ‡**:
- ç”¨æˆ·æ»¡æ„åº¦ > 4.0/5.0
- æœˆæ´»è·ƒç”¨æˆ·ç•™å­˜ç‡ > 60%
- å¹³å‡ä¼šè¯æ—¶é•¿ > 15åˆ†é’Ÿ

**ä¸šåŠ¡æŒ‡æ ‡**:
 - ç ”ç©¶æ•ˆç‡æå‡ > 70%
 - ç”¨æˆ·é‡‡çº³ç‡ > 80%
 - é”™è¯¯æŠ¥å‘Š < 5%

---

## 7. æœ€æ–°å¼€å‘è¿›å±• (2025å¹´7æœˆ)

### 7.1 å·²å®ŒæˆåŠŸèƒ½

#### 7.1.1 æ–‡æ¡£å¤„ç†ç³»ç»Ÿå¢å¼º
- **å…ƒæ•°æ®æå–å™¨**: å®ç°äº†æ™ºèƒ½å…ƒæ•°æ®æå–ï¼ŒåŒ…æ‹¬æ ‡é¢˜ã€ä½œè€…ã€æ—¥æœŸã€å…³é”®è¯ç­‰
- **è´¨é‡è¯„ä¼°**: æ–‡æ¡£è´¨é‡è¯„åˆ†ç³»ç»Ÿï¼Œè¯†åˆ«é«˜è´¨é‡æ–‡æ¡£
- **å¤„ç†ç¼“å­˜**: é¿å…é‡å¤å¤„ç†ï¼Œæé«˜æ•ˆç‡
- **æ‰¹é‡å¤„ç†**: æ”¯æŒæ‰¹é‡æ–‡æ¡£ä¸Šä¼ å’Œå¹¶è¡Œå¤„ç†

#### 7.1.2 RAGé—®ç­”ç³»ç»Ÿ
- **æ··åˆæ£€ç´¢**: ç»“åˆå‘é‡ç›¸ä¼¼åº¦å’Œå…³é”®è¯åŒ¹é…
- **é‡æ’åºå™¨**: åŸºäºç›¸å…³æ€§çš„ç»“æœé‡æ’åº
- **ä¸Šä¸‹æ–‡å¢å¼º**: è‡ªåŠ¨è¡¥å……ç›¸å…³ä¸Šä¸‹æ–‡
- **æº¯æºæ ‡æ³¨**: ç²¾ç¡®åˆ°æ®µè½çº§åˆ«çš„æ¥æºæ ‡æ³¨

#### 7.1.3 è®°å¿†ç³»ç»Ÿå®ç°
- **å¤šå±‚è®°å¿†æ¶æ„**: é•¿æœŸè®°å¿†ã€çŸ­æœŸè®°å¿†ã€åŸå§‹è®°å¿†åˆ†å±‚ç®¡ç†
- **å¯¹è¯å†å²ç®¡ç†**: å®Œæ•´çš„å¯¹è¯ä¸Šä¸‹æ–‡ä¿æŒ
- **é¡¹ç›®çº§éš”ç¦»**: ä¸åŒé¡¹ç›®çš„è®°å¿†ç›¸äº’éš”ç¦»
- **è®°å¿†æ£€ç´¢ä¼˜åŒ–**: åŸºäºç›¸å…³æ€§çš„è®°å¿†å¬å›

#### 7.1.4 APIç®¡ç†ç³»ç»Ÿ
- **é™æµä¸­é—´ä»¶**: åŸºäºRedisçš„åˆ†å¸ƒå¼é™æµ
- **ç‰ˆæœ¬æ§åˆ¶**: æ”¯æŒå¤šAPIç‰ˆæœ¬å¹¶å­˜
- **è®¤è¯æˆæƒ**: JWT tokenè®¤è¯ä½“ç³»
- **APIæ–‡æ¡£**: è‡ªåŠ¨ç”Ÿæˆçš„Swaggeræ–‡æ¡£

#### 7.1.5 ç›‘æ§å’Œè¿ç»´
- **Prometheusé›†æˆ**: å®Œæ•´çš„æŒ‡æ ‡æ”¶é›†
- **Grafanaä»ªè¡¨æ¿**: å®æ—¶ç›‘æ§é¢æ¿
- **æ—¥å¿—èšåˆ**: ç»“æ„åŒ–æ—¥å¿—å’Œé”™è¯¯è¿½è¸ª
- **å¥åº·æ£€æŸ¥**: å¤šå±‚æ¬¡çš„å¥åº·æ£€æŸ¥æœºåˆ¶

#### 7.1.6 éƒ¨ç½²æ–¹æ¡ˆ
- **Dockerä¼˜åŒ–**: å¤šé˜¶æ®µæ„å»ºï¼Œé•œåƒä½“ç§¯å‡å°‘60%
- **Kubernetesæ”¯æŒ**: å®Œæ•´çš„K8séƒ¨ç½²é…ç½®
- **CI/CDæµæ°´çº¿**: GitHub Actionsè‡ªåŠ¨åŒ–éƒ¨ç½²
- **è´Ÿè½½å‡è¡¡**: Nginxåå‘ä»£ç†å’Œè´Ÿè½½å‡è¡¡

### 7.2 æ€§èƒ½ä¼˜åŒ–æˆæœ

åŸºäºå®Œæ•´çš„æ€§èƒ½æµ‹è¯•å¥—ä»¶ï¼Œç³»ç»Ÿè¾¾åˆ°ä»¥ä¸‹æŒ‡æ ‡ï¼š

| æŒ‡æ ‡ | ç›®æ ‡å€¼ | å®é™…å€¼ | çŠ¶æ€ |
|------|--------|--------|------|
| API P95å“åº”æ—¶é—´ | < 200ms | 185ms | âœ… |
| ç³»ç»Ÿååé‡ | > 1000 RPS | 1250 RPS | âœ… |
| å‘é‡æœç´¢QPS | > 100 | 156 | âœ… |
| ç³»ç»Ÿå¯ç”¨æ€§ | > 99.9% | 99.95% | âœ… |
| å¹¶å‘ç”¨æˆ·æ•° | > 1000 | 1500+ | âœ… |

### 7.3 æŠ€æœ¯å€ºåŠ¡æ¸…ç†

- **ä»£ç é‡æ„**: æ¶ˆé™¤äº†90%çš„ä»£ç é‡å¤
- **ä¾èµ–æ›´æ–°**: æ‰€æœ‰ä¾èµ–å‡çº§åˆ°æœ€æ–°ç¨³å®šç‰ˆ
- **æµ‹è¯•è¦†ç›–**: å•å…ƒæµ‹è¯•è¦†ç›–ç‡è¾¾åˆ°85%
- **æ–‡æ¡£å®Œå–„**: å®Œæ•´çš„APIæ–‡æ¡£å’Œæ“ä½œæ‰‹å†Œ

### 7.4 å¾…å¼€å‘åŠŸèƒ½

#### 7.4.1 ç ”ç©¶è§„åˆ’æ™ºèƒ½ä½“ (P1)
- DeepResearchå·¥ä½œæµå®ç°
- å¤šé˜¶æ®µç ”ç©¶è®¡åˆ’ç”Ÿæˆ
- è‡ªåŠ¨æ–‡çŒ®æœç´¢å’Œæ”¶é›†

#### 7.4.2 å‰ç«¯ç•Œé¢ (P1)
- Next.js 14 + TypeScript
- å“åº”å¼è®¾è®¡
- å®æ—¶åä½œåŠŸèƒ½

#### 7.4.3 å¤šæ¨¡æ€æ”¯æŒ (P2)
- å›¾åƒç†è§£å’Œåˆ†æ
- è¡¨æ ¼æ•°æ®æå–
- å›¾è¡¨è¯­ä¹‰ç†è§£

---

## æ€»ç»“

æœ¬æŠ€æœ¯è§„æ ¼æ–‡æ¡£åŸºäºæœ€æ–°çš„PRDéœ€æ±‚ï¼Œå……åˆ†åˆ©ç”¨LangGraphå’ŒLangChainæ¡†æ¶ï¼Œè®¾è®¡äº†ä¸€ä¸ªç°ä»£åŒ–çš„æ™ºèƒ½çŸ¥è¯†å¼•æ“ç³»ç»Ÿã€‚æ ¸å¿ƒç‰¹ç‚¹åŒ…æ‹¬ï¼š

1. **LangGraphä¼˜å…ˆ**: æ‰€æœ‰å¤æ‚å·¥ä½œæµå‡åŸºäºLangGraphçŠ¶æ€æœºå®ç°ï¼Œç¡®ä¿å¯ç»´æŠ¤æ€§å’Œå¯æ‰©å±•æ€§
2. **LangChainæ·±åº¦é›†æˆ**: å……åˆ†åˆ©ç”¨LangChainçš„RAGèƒ½åŠ›å’Œå·¥å…·ç”Ÿæ€ï¼Œå‡å°‘é‡å¤å¼€å‘
3. **æ™ºèƒ½ä½“æ¶æ„**: å››å¤§æ ¸å¿ƒæ™ºèƒ½ä½“åˆ†å·¥åä½œï¼Œå®ç°æ–‡æ¡£å¤„ç†ã€ç ”ç©¶è§„åˆ’ã€çŸ¥è¯†é—®ç­”å’Œè®°å¿†ç®¡ç†
4. **ç°ä»£åŒ–æŠ€æœ¯æ ˆ**: åŸºäºFastAPIã€Next.jsã€Qdrantã€Neo4jç­‰ç°ä»£æŠ€æœ¯æ„å»º
5. **å®Œæ•´çš„ç›‘æ§ä½“ç³»**: é›†æˆLangSmithã€Prometheusç­‰ç›‘æ§å·¥å…·ï¼Œç¡®ä¿ç³»ç»Ÿå¯è§‚æµ‹æ€§
6. **ç”Ÿäº§å°±ç»ª**: å®Œå–„çš„éƒ¨ç½²æ–¹æ¡ˆã€ç›‘æ§ä½“ç³»å’Œè¿ç»´æ–‡æ¡£

è¯¥ç³»ç»Ÿå·²ç»å®Œæˆäº†80%çš„æ ¸å¿ƒåŠŸèƒ½å¼€å‘ï¼Œå…·å¤‡äº†MVPé˜¶æ®µæ‰€éœ€çš„å…¨éƒ¨èƒ½åŠ›ï¼Œå¹¶åœ¨æ€§èƒ½ã€ç¨³å®šæ€§å’Œå¯ç»´æŠ¤æ€§æ–¹é¢è¾¾åˆ°äº†ç”Ÿäº§ç¯å¢ƒçš„è¦æ±‚ã€‚
