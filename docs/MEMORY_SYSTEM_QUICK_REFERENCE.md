# DPAè®°å¿†ç³»ç»Ÿ - å¿«é€Ÿå‚è€ƒæŒ‡å—

## æ ¸å¿ƒæ¦‚å¿µé€ŸæŸ¥

### ğŸ§  å››å±‚è®°å¿†æ¶æ„
```python
memory_layers = {
    "sensory": "åŸå§‹è¾“å…¥ç¼“å†² (<1ç§’)",
    "working": "å½“å‰ä»»åŠ¡ä¸Šä¸‹æ–‡ (7Â±2é¡¹)",
    "episodic": "å…·ä½“äº‹ä»¶å’Œç»å† (ä¸­æœŸ)",
    "semantic": "æ¦‚å¿µå’Œäº‹å®çŸ¥è¯† (é•¿æœŸ)"
}
```

### ğŸ”„ è®¤çŸ¥å¾ªç¯
```
æ„ŸçŸ¥ â†’ æ³¨æ„ â†’ ç¼–ç  â†’ å­˜å‚¨ â†’ æ£€ç´¢ â†’ æ¨ç† â†’ è§„åˆ’ â†’ æ‰§è¡Œ â†’ åæ€
```

### ğŸ“ è®°å¿†åº“ç»“æ„
```
memory-bank/
â”œâ”€â”€ metadata.json          # å…ƒæ•°æ®
â”œâ”€â”€ source_documents.md    # æºæ–‡æ¡£
â”œâ”€â”€ key_concepts.md       # å…³é”®æ¦‚å¿µ
â”œâ”€â”€ dynamic_summary.md    # åŠ¨æ€æ‘˜è¦
â”œâ”€â”€ learning_journal/     # å­¦ä¹ æ—¥å¿—
â”œâ”€â”€ hypotheses/          # å‡è®¾éªŒè¯
â””â”€â”€ agent_rules.md       # æ™ºèƒ½ä½“è§„åˆ™
```

## å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒè®¾ç½®
```bash
# å…‹éš†é¡¹ç›®
git clone <repo>
cd DPA

# åˆ›å»ºç¯å¢ƒ
conda env create -f environment.yml
conda activate dpa_gen

# å®‰è£…é¢å¤–ä¾èµ–
pip install langgraph==0.2.0
pip install torch torch-geometric
```

### 2. é…ç½®æ•°æ®åº“
```bash
# PostgreSQL
createdb dpa_cognitive
psql dpa_cognitive < scripts/init_postgres.sql

# Neo4j
neo4j-admin import --database=dpa --nodes=init/nodes.csv --relationships=init/rels.csv

# Redis
redis-server --appendonly yes
```

### 3. æœ€å°ç¤ºä¾‹
```python
from src.core.memory.workflow import build_cognitive_workflow
from src.core.memory.state import DPACognitiveState

# æ„å»ºå·¥ä½œæµ
app = build_cognitive_workflow()

# åˆå§‹çŠ¶æ€
initial_state = {
    "messages": [HumanMessage(content="åˆ†æè¿™ä»½æ–‡æ¡£")],
    "current_documents": [document],
    "user_id": "researcher_001",
    "project_id": "project_001"
}

# æ‰§è¡Œ
config = {"configurable": {"thread_id": "session_001"}}
result = await app.ainvoke(initial_state, config)
```

## å…³é”®ç»„ä»¶

### 1. S2è¯­ä¹‰åˆ†å—
```python
from src.core.chunking.s2_chunker import S2SemanticChunker

chunker = S2SemanticChunker()
chunks = await chunker.chunk_document(
    document_text,
    metadata={"title": "...", "type": "..."}
)
```

### 2. æ··åˆæ£€ç´¢
```python
from src.services.hybrid_retrieval import EnhancedHybridRetriever

retriever = EnhancedHybridRetriever()
results = await retriever.hybrid_retrieve(
    query="æ·±åº¦å­¦ä¹ åœ¨NLPä¸­çš„åº”ç”¨",
    state=current_state
)
```

### 3. GNNå­¦ä¹ 
```python
from src.learning.gnn_completion import GNNKnowledgeCompletion

gnn = GNNKnowledgeCompletion()
hypotheses = await gnn.predict_missing_links(
    knowledge_graph,
    confidence_threshold=0.8
)
```

### 4. è®°å¿†åº“ç®¡ç†
```python
from src.core.memory.memory_bank import MemoryBankManager

memory_bank = MemoryBankManager()
await memory_bank.read_verify_update_execute(state)
```

## å¸¸ç”¨å‘½ä»¤

### å¼€å‘
```bash
# è¿è¡Œæµ‹è¯•
pytest tests/test_memory_system.py -v

# å¯åŠ¨æœåŠ¡
uvicorn src.api.main:app --reload

# ç›‘æ§æ€§èƒ½
python scripts/monitor_performance.py
```

### æ•°æ®ç®¡ç†
```bash
# å¤‡ä»½è®°å¿†åº“
tar -czf memory-bank-backup.tar.gz memory-bank/

# å¯¼å‡ºçŸ¥è¯†å›¾è°±
python scripts/export_knowledge_graph.py

# æ¸…ç†è¿‡æœŸè®°å¿†
python scripts/cleanup_expired_memories.py
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **å†…å­˜æº¢å‡º**
```python
# æ£€æŸ¥å·¥ä½œè®°å¿†å¤§å°
if len(state["working_memory"]) > 9:
    state = StateManager.compress_working_memory(state)
```

2. **æŸ¥è¯¢è¶…æ—¶**
```python
# ä½¿ç”¨ç¼“å­˜
results = await cache.get_or_compute(
    key=query_hash,
    compute_fn=lambda: retriever.search(query),
    ttl=1800
)
```

3. **å›¾è°±æŸ¥è¯¢æ…¢**
```cypher
// ç¡®ä¿æœ‰åˆé€‚çš„ç´¢å¼•
CREATE INDEX ON :Concept(name);
CREATE INDEX ON :Concept(embedding_id);
```

## æ€§èƒ½ä¼˜åŒ–æŠ€å·§

1. **æ‰¹é‡å¤„ç†**
```python
# æ‰¹é‡åµŒå…¥
embeddings = await batch_embed(texts, batch_size=50)

# æ‰¹é‡å›¾æŸ¥è¯¢
results = await batch_graph_query(queries, max_concurrent=10)
```

2. **å¼‚æ­¥å¹¶å‘**
```python
# å¹¶è¡Œæ£€ç´¢
tasks = [
    vector_search(query),
    graph_search(query),
    memory_bank_search(query)
]
results = await asyncio.gather(*tasks)
```

3. **æ™ºèƒ½ç¼“å­˜**
```python
# å¤šçº§ç¼“å­˜ç­–ç•¥
cache_config = {
    "local": {"size": 1000, "ttl": 300},
    "redis": {"ttl": 3600},
    "preload": ["hot_concepts", "frequent_queries"]
}
```

## ç›‘æ§æŒ‡æ ‡

```python
# PrometheusæŒ‡æ ‡
metrics = {
    "memory_operations_total": Counter,
    "query_latency_seconds": Histogram,
    "knowledge_graph_nodes": Gauge,
    "learning_progress": Gauge
}

# å¥åº·æ£€æŸ¥ç«¯ç‚¹
GET /health
GET /metrics
GET /memory-stats
```

## æ‰©å±•ç‚¹

### æ·»åŠ æ–°çš„è®°å¿†ç±»å‹
```python
class CustomMemoryLayer:
    def encode(self, data): ...
    def store(self, encoded): ...
    def retrieve(self, query): ...
    def decay(self, time_delta): ...
```

### è‡ªå®šä¹‰å­¦ä¹ ç­–ç•¥
```python
class CustomLearningStrategy:
    def identify_gaps(self, state): ...
    def generate_plan(self, gaps): ...
    def execute_step(self, step): ...
```

### é›†æˆå¤–éƒ¨å·¥å…·
```python
@tool
def custom_research_tool(query: str) -> str:
    """è‡ªå®šä¹‰ç ”ç©¶å·¥å…·"""
    # å®ç°é€»è¾‘
    return results
```

## ç›¸å…³èµ„æº

- [å®Œæ•´è®¾è®¡æ–‡æ¡£](./MEMORY_SYSTEM_DESIGN_V3_FINAL.md)
- [å®æ–½è·¯çº¿å›¾](./MEMORY_SYSTEM_IMPLEMENTATION_ROADMAP.md)
- [APIæ–‡æ¡£](http://localhost:8000/docs)
- [ç›‘æ§é¢æ¿](http://localhost:3000/d/memory-system)

---

ğŸ’¡ **æç¤º**ï¼šå…ˆä»æœ€å°ç¤ºä¾‹å¼€å§‹ï¼Œé€æ­¥æ·»åŠ é«˜çº§åŠŸèƒ½ã€‚è®°å¾—ç»å¸¸æŸ¥çœ‹è®°å¿†åº“æ–‡ä»¶ï¼Œäº†è§£ç³»ç»Ÿçš„"æ€è€ƒè¿‡ç¨‹"ã€‚