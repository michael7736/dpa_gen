#!/usr/bin/env python3
"""
MVPç®€åŒ–æ¼”ç¤ºè„šæœ¬
å±•ç¤ºæ ¸å¿ƒåŠŸèƒ½ï¼Œè·³è¿‡æ•°æ®åº“å­˜å‚¨
"""
import asyncio
import sys
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from src.core.memory.memory_bank_manager import create_memory_bank_manager
from src.core.memory.mvp_state import MVPCognitiveState, create_initial_state
from src.utils.logging_utils import get_logger

logger = get_logger(__name__)


async def demo_memory_bank():
    """æ¼”ç¤ºMemory BankåŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸ¦ Memory Bankæ¼”ç¤º")
    print("="*60)
    
    manager = create_memory_bank_manager()
    project_id = "demo_project"
    
    # åˆå§‹åŒ–é¡¹ç›®
    print("\n1. åˆå§‹åŒ–é¡¹ç›®...")
    await manager.initialize_project(project_id)
    print("âœ… é¡¹ç›®åˆå§‹åŒ–å®Œæˆ")
    
    # æ›´æ–°ä¸Šä¸‹æ–‡
    print("\n2. æ›´æ–°é¡¹ç›®ä¸Šä¸‹æ–‡...")
    await manager.update_context(
        project_id=project_id,
        new_content="""
æ·±åº¦å­¦ä¹ é¡¹ç›®ç¬”è®°ï¼š
- å·²ç»å­¦ä¹ äº†CNNçš„åŸºæœ¬æ¦‚å¿µ
- äº†è§£äº†RNNåœ¨åºåˆ—å¤„ç†ä¸­çš„åº”ç”¨
- Transformeræ˜¯å½“å‰NLPçš„ä¸»æµæ¶æ„
        """.strip(),
        source="learning_notes"
    )
    print("âœ… ä¸Šä¸‹æ–‡å·²æ›´æ–°")
    
    # æ·»åŠ æ¦‚å¿µ
    print("\n3. æ·»åŠ æ ¸å¿ƒæ¦‚å¿µ...")
    concepts = [
        {
            "name": "CNN",
            "category": "neural_network",
            "description": "å·ç§¯ç¥ç»ç½‘ç»œï¼Œä¸“é—¨ç”¨äºå›¾åƒå¤„ç†",
            "confidence": 0.9
        },
        {
            "name": "RNN",
            "category": "neural_network",
            "description": "å¾ªç¯ç¥ç»ç½‘ç»œï¼Œå¤„ç†åºåˆ—æ•°æ®",
            "confidence": 0.85
        },
        {
            "name": "Transformer",
            "category": "neural_network",
            "description": "åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„æ¶æ„",
            "confidence": 0.95
        }
    ]
    
    await manager.add_concepts(project_id, concepts)
    print(f"âœ… æ·»åŠ äº† {len(concepts)} ä¸ªæ¦‚å¿µ")
    
    # æ·»åŠ å­¦ä¹ è®°å½•
    print("\n4. æ·»åŠ å­¦ä¹ è®°å½•...")
    await manager.add_learning_entry(
        project_id=project_id,
        content="å®Œæˆäº†æ·±åº¦å­¦ä¹ åŸºç¡€æ¶æ„çš„å­¦ä¹ ",
        learning_type="milestone",
        metadata={"progress": "30%"}
    )
    print("âœ… å­¦ä¹ è®°å½•å·²æ·»åŠ ")
    
    # è·å–å¿«ç…§
    print("\n5. è·å–Memory Bankå¿«ç…§...")
    snapshot = await manager.get_snapshot(project_id)
    
    if snapshot:
        print("âœ… Memory BankçŠ¶æ€:")
        print(f"   - é¡¹ç›®ID: {snapshot.project_id}")
        print(f"   - ä¸Šä¸‹æ–‡å¤§å°: {len(snapshot.context)} å­—ç¬¦")
        print(f"   - åŠ¨æ€æ‘˜è¦: {snapshot.dynamic_summary[:100]}...")
        print(f"   - æ ¸å¿ƒæ¦‚å¿µæ•°: {len(snapshot.core_concepts)}")
        print(f"   - å­¦ä¹ è®°å½•æ•°: {len(snapshot.learning_journals)}")
        
        print("\n   æ ¸å¿ƒæ¦‚å¿µåˆ—è¡¨:")
        for concept in snapshot.core_concepts[:3]:
            print(f"   â€¢ {concept['name']}: {concept['description']}")


async def demo_cognitive_state():
    """æ¼”ç¤ºè®¤çŸ¥çŠ¶æ€ç®¡ç†"""
    print("\n" + "="*60)
    print("ğŸ§  è®¤çŸ¥çŠ¶æ€æ¼”ç¤º")
    print("="*60)
    
    # åˆ›å»ºåˆå§‹çŠ¶æ€
    state = create_initial_state(
        input_text="è§£é‡Šæ·±åº¦å­¦ä¹ ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶",
        project_id="demo_project",
        user_id="u1"
    )
    
    print(f"\nåˆå§‹çŠ¶æ€:")
    print(f"   - è¾“å…¥: {state['input']}")
    print(f"   - é¡¹ç›®ID: {state['project_id']}")
    print(f"   - ç”¨æˆ·ID: {state['user_id']}")
    print(f"   - é”™è¯¯åˆ—è¡¨: {state['errors']}")
    
    # æ¨¡æ‹Ÿè®¤çŸ¥æµç¨‹
    print("\næ¨¡æ‹Ÿ5èŠ‚ç‚¹è®¤çŸ¥æµç¨‹:")
    
    # 1. æ„ŸçŸ¥é˜¶æ®µ
    state['perceived_input'] = "ç”¨æˆ·æƒ³äº†è§£æ·±åº¦å­¦ä¹ ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶æ¦‚å¿µ"
    print("   âœ… 1. æ„ŸçŸ¥: ç†è§£ç”¨æˆ·æ„å›¾")
    
    # 2. å¤„ç†é˜¶æ®µ
    state['processed_input'] = {
        "topic": "attention_mechanism",
        "domain": "deep_learning",
        "intent": "explain"
    }
    print("   âœ… 2. å¤„ç†: æå–å…³é”®ä¿¡æ¯")
    
    # 3. æ£€ç´¢é˜¶æ®µ
    state['retrieved_context'] = [
        "æ³¨æ„åŠ›æœºåˆ¶å…è®¸æ¨¡å‹å…³æ³¨è¾“å…¥çš„ä¸åŒéƒ¨åˆ†",
        "Transformerä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶",
        "æ³¨æ„åŠ›æƒé‡è¡¨ç¤ºé‡è¦æ€§åˆ†æ•°"
    ]
    print("   âœ… 3. æ£€ç´¢: æ‰¾åˆ°ç›¸å…³ä¸Šä¸‹æ–‡")
    
    # 4. æ¨ç†é˜¶æ®µ
    state['reasoning_result'] = """
æ³¨æ„åŠ›æœºåˆ¶æ˜¯æ·±åº¦å­¦ä¹ ä¸­çš„ä¸€ä¸ªé‡è¦æ¦‚å¿µï¼Œå®ƒå…è®¸æ¨¡å‹åœ¨å¤„ç†è¾“å…¥æ—¶åŠ¨æ€åœ°å…³æ³¨ä¸åŒéƒ¨åˆ†ã€‚
ä¸»è¦ç‰¹ç‚¹åŒ…æ‹¬ï¼š
1. è®¡ç®—æ³¨æ„åŠ›æƒé‡æ¥è¡¨ç¤ºä¸åŒè¾“å…¥éƒ¨åˆ†çš„é‡è¦æ€§
2. åœ¨Transformeræ¶æ„ä¸­è¢«å¹¿æ³›åº”ç”¨
3. æé«˜äº†æ¨¡å‹å¯¹é•¿åºåˆ—çš„å¤„ç†èƒ½åŠ›
    """.strip()
    print("   âœ… 4. æ¨ç†: ç”Ÿæˆå›ç­”")
    
    # 5. è®°å¿†æ›´æ–°
    state['memory_updated'] = True
    print("   âœ… 5. æ›´æ–°: ä¿å­˜åˆ°è®°å¿†ç³»ç»Ÿ")
    
    print("\nè®¤çŸ¥æµç¨‹å®Œæˆï¼")


async def demo_document_chunking():
    """æ¼”ç¤ºæ–‡æ¡£åˆ†å—"""
    print("\n" + "="*60)
    print("ğŸ“„ æ–‡æ¡£åˆ†å—æ¼”ç¤º")
    print("="*60)
    
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    
    # åˆ›å»ºç¤ºä¾‹æ–‡æ¡£
    document_text = """
æ·±åº¦å­¦ä¹ åŸºç¡€æ•™ç¨‹

ç¬¬ä¸€ç« ï¼šç¥ç»ç½‘ç»œç®€ä»‹
ç¥ç»ç½‘ç»œæ˜¯æ·±åº¦å­¦ä¹ çš„åŸºç¡€ï¼Œå®ƒæ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ï¼Œé€šè¿‡å¤šå±‚ç¥ç»å…ƒæ¥å­¦ä¹ æ•°æ®çš„ç‰¹å¾è¡¨ç¤ºã€‚

ç¬¬äºŒç« ï¼šå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰
CNNåœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«é€‚åˆå¤„ç†å›¾åƒæ•°æ®ã€‚å®ƒä½¿ç”¨å·ç§¯å±‚ã€æ± åŒ–å±‚å’Œå…¨è¿æ¥å±‚çš„ç»„åˆã€‚

ç¬¬ä¸‰ç« ï¼šå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰
RNNä¸“é—¨å¤„ç†åºåˆ—æ•°æ®ï¼Œå¦‚æ–‡æœ¬å’Œæ—¶é—´åºåˆ—ã€‚LSTMå’ŒGRUæ˜¯RNNçš„æ”¹è¿›ç‰ˆæœ¬ï¼Œè§£å†³äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚

ç¬¬å››ç« ï¼šTransformeræ¶æ„
Transformerå½»åº•æ”¹å˜äº†NLPé¢†åŸŸï¼Œå®ƒä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯ä»¥å¹¶è¡Œå¤„ç†åºåˆ—æ•°æ®ï¼Œå¤§å¤§æé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚
    """.strip()
    
    # åˆ›å»ºæ–‡æœ¬åˆ†å‰²å™¨
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=200,
        chunk_overlap=50,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " ", ""]
    )
    
    # åˆ†å‰²æ–‡æ¡£
    chunks = text_splitter.split_text(document_text)
    
    print(f"\næ–‡æ¡£åˆ†å—ç»“æœ:")
    print(f"   - åŸæ–‡é•¿åº¦: {len(document_text)} å­—ç¬¦")
    print(f"   - åˆ†å—æ•°é‡: {len(chunks)}")
    print(f"   - åˆ†å—å¤§å°: 200 å­—ç¬¦")
    print(f"   - é‡å å¤§å°: 50 å­—ç¬¦")
    
    print("\nåˆ†å—å†…å®¹é¢„è§ˆ:")
    for i, chunk in enumerate(chunks[:3]):
        print(f"\n   å— {i+1}:")
        print(f"   {chunk[:100]}...")
        print(f"   (é•¿åº¦: {len(chunk)} å­—ç¬¦)")


async def demo_embedding_generation():
    """æ¼”ç¤ºåµŒå…¥å‘é‡ç”Ÿæˆï¼ˆæ¨¡æ‹Ÿï¼‰"""
    print("\n" + "="*60)
    print("ğŸ”¢ åµŒå…¥å‘é‡æ¼”ç¤º")
    print("="*60)
    
    import numpy as np
    
    # æ¨¡æ‹Ÿæ–‡æœ¬
    texts = [
        "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯",
        "CNNé€‚åˆå¤„ç†å›¾åƒæ•°æ®",
        "RNNé€‚åˆå¤„ç†åºåˆ—æ•°æ®"
    ]
    
    print("\nç”ŸæˆåµŒå…¥å‘é‡ï¼ˆæ¨¡æ‹Ÿï¼‰:")
    embeddings = []
    
    for i, text in enumerate(texts):
        # æ¨¡æ‹Ÿç”Ÿæˆ1536ç»´å‘é‡
        embedding = np.random.randn(1536)
        embedding = embedding / np.linalg.norm(embedding)  # å½’ä¸€åŒ–
        embeddings.append(embedding)
        
        print(f"\n   æ–‡æœ¬ {i+1}: {text}")
        print(f"   å‘é‡ç»´åº¦: {len(embedding)}")
        print(f"   å‘é‡èŒƒæ•°: {np.linalg.norm(embedding):.4f}")
        print(f"   å‰5ä¸ªå€¼: {embedding[:5].round(4)}")
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    print("\n\nç›¸ä¼¼åº¦è®¡ç®—ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰:")
    for i in range(len(texts)):
        for j in range(i+1, len(texts)):
            similarity = np.dot(embeddings[i], embeddings[j])
            print(f"   æ–‡æœ¬{i+1} vs æ–‡æœ¬{j+1}: {similarity:.4f}")


async def main():
    """ä¸»æ¼”ç¤ºæµç¨‹"""
    print("\n" + "="*80)
    print("ğŸš€ DPA MVPæ ¸å¿ƒåŠŸèƒ½æ¼”ç¤ºï¼ˆç®€åŒ–ç‰ˆï¼‰")
    print("="*80)
    print(f"æ¼”ç¤ºæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        # 1. Memory Bankæ¼”ç¤º
        await demo_memory_bank()
        await asyncio.sleep(0.5)
        
        # 2. è®¤çŸ¥çŠ¶æ€æ¼”ç¤º
        await demo_cognitive_state()
        await asyncio.sleep(0.5)
        
        # 3. æ–‡æ¡£åˆ†å—æ¼”ç¤º
        await demo_document_chunking()
        await asyncio.sleep(0.5)
        
        # 4. åµŒå…¥å‘é‡æ¼”ç¤º
        await demo_embedding_generation()
        
        print("\n" + "="*80)
        print("âœ… MVPæ ¸å¿ƒåŠŸèƒ½æ¼”ç¤ºå®Œæˆï¼")
        print("="*80)
        
        print("\nå·²æ¼”ç¤ºçš„æ ¸å¿ƒåŠŸèƒ½:")
        print("1. âœ… Memory Bank - æŒä¹…åŒ–è®°å¿†ç®¡ç†")
        print("2. âœ… è®¤çŸ¥çŠ¶æ€ - 5èŠ‚ç‚¹å·¥ä½œæµçŠ¶æ€")
        print("3. âœ… æ–‡æ¡£åˆ†å— - é€’å½’å­—ç¬¦åˆ†å‰²")
        print("4. âœ… åµŒå…¥å‘é‡ - å‘é‡åŒ–å’Œç›¸ä¼¼åº¦")
        
        print("\nğŸ’¡ è¯´æ˜:")
        print("   - è¿™æ˜¯ç®€åŒ–æ¼”ç¤ºç‰ˆæœ¬ï¼Œè·³è¿‡äº†æ•°æ®åº“å­˜å‚¨")
        print("   - å®Œæ•´ç‰ˆæœ¬éœ€è¦é…ç½®PostgreSQLã€Qdrantã€Neo4jç­‰æœåŠ¡")
        print("   - æ ¸å¿ƒç®—æ³•å’Œæ•°æ®ç»“æ„å·²ç»å®ç°")
        
    except Exception as e:
        logger.error(f"æ¼”ç¤ºå‡ºé”™: {e}")
        print(f"\nâŒ æ¼”ç¤ºå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    asyncio.run(main())