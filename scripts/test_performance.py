#!/usr/bin/env python3
"""
æ€§èƒ½æµ‹è¯•è„šæœ¬
æµ‹è¯•ç³»ç»Ÿçš„å“åº”æ—¶é—´ã€å¹¶å‘èƒ½åŠ›å’Œèµ„æºä½¿ç”¨
"""

import asyncio
import httpx
import time
import statistics
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor

# é…ç½®
BASE_URL = "http://localhost:8200"
USER_ID = "u1"
HEADERS = {"X-USER-ID": USER_ID}

# æµ‹è¯•ç»“æœ
performance_results = {
    "response_times": [],
    "concurrent_results": [],
    "memory_usage": [],
    "errors": []
}


def print_header(title: str):
    """æ‰“å°æµ‹è¯•æ ‡é¢˜"""
    print("\n" + "=" * 60)
    print(f"âš¡ {title}")
    print("=" * 60)


def print_metric(name: str, value: float, unit: str = "ms", threshold: float = None):
    """æ‰“å°æ€§èƒ½æŒ‡æ ‡"""
    status = "âœ…" if threshold is None or value <= threshold else "âš ï¸"
    print(f"{status} {name}: {value:.1f}{unit}")
    if threshold and value > threshold:
        print(f"   è­¦å‘Š: è¶…è¿‡é˜ˆå€¼ {threshold}{unit}")


async def test_response_times(client: httpx.AsyncClient):
    """æµ‹è¯•å“åº”æ—¶é—´"""
    print_header("å“åº”æ—¶é—´æµ‹è¯•")
    
    endpoints = [
        ("å¥åº·æ£€æŸ¥", "GET", "/health", None),
        ("é¡¹ç›®åˆ—è¡¨", "GET", "/api/v1/projects", HEADERS),
        ("AAGå¿«é€Ÿç•¥è¯»", "POST", "/api/v1/aag/skim", {
            **HEADERS,
            "Content-Type": "application/json"
        }),
        ("åŸºç¡€é—®ç­”", "POST", "/api/v1/qa/answer", {
            **HEADERS,
            "Content-Type": "application/json"
        })
    ]
    
    for name, method, endpoint, headers in endpoints:
        times = []
        errors = 0
        
        # è¿è¡Œ5æ¬¡æµ‹è¯•å–å¹³å‡å€¼
        for i in range(5):
            try:
                start_time = time.time()
                
                if method == "GET":
                    response = await client.get(f"{BASE_URL}{endpoint}", headers=headers or {})
                else:
                    test_data = {}
                    if "skim" in endpoint:
                        test_data = {
                            "skim_request": {
                                "document_id": "perf_test",
                                "document_content": "æ€§èƒ½æµ‹è¯•æ–‡æ¡£å†…å®¹",
                                "document_type": "test"
                            }
                        }
                    elif "qa" in endpoint:
                        test_data = {
                            "question": "æ€§èƒ½æµ‹è¯•é—®é¢˜",
                            "project_id": "test_project"
                        }
                    
                    response = await client.post(
                        f"{BASE_URL}{endpoint}",
                        headers=headers or {},
                        json=test_data
                    )
                
                end_time = time.time()
                response_time = (end_time - start_time) * 1000
                
                if response.status_code in [200, 201]:
                    times.append(response_time)
                else:
                    errors += 1
                    
            except Exception as e:
                errors += 1
                performance_results["errors"].append(f"{name}: {str(e)}")
        
        if times:
            avg_time = statistics.mean(times)
            min_time = min(times)
            max_time = max(times)
            
            print(f"\n{name}:")
            print_metric("  å¹³å‡å“åº”æ—¶é—´", avg_time, "ms", 2000)
            print_metric("  æœ€å¿«å“åº”æ—¶é—´", min_time, "ms")
            print_metric("  æœ€æ…¢å“åº”æ—¶é—´", max_time, "ms")
            
            if errors > 0:
                print(f"  âŒ é”™è¯¯æ¬¡æ•°: {errors}/5")
            
            performance_results["response_times"].append({
                "endpoint": name,
                "avg_time": avg_time,
                "min_time": min_time,
                "max_time": max_time,
                "errors": errors
            })


async def test_concurrent_requests(client: httpx.AsyncClient):
    """æµ‹è¯•å¹¶å‘è¯·æ±‚"""
    print_header("å¹¶å‘æ€§èƒ½æµ‹è¯•")
    
    async def make_request():
        """å‘èµ·å•ä¸ªè¯·æ±‚"""
        try:
            start_time = time.time()
            response = await client.get(f"{BASE_URL}/health")
            end_time = time.time()
            
            return {
                "success": response.status_code == 200,
                "response_time": (end_time - start_time) * 1000,
                "status_code": response.status_code
            }
        except Exception as e:
            return {
                "success": False,
                "response_time": 0,
                "error": str(e)
            }
    
    # æµ‹è¯•ä¸åŒå¹¶å‘çº§åˆ«
    concurrency_levels = [1, 5, 10, 20]
    
    for concurrency in concurrency_levels:
        print(f"\næµ‹è¯•å¹¶å‘çº§åˆ«: {concurrency}")
        
        start_time = time.time()
        
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        tasks = [make_request() for _ in range(concurrency)]
        results = await asyncio.gather(*tasks)
        
        end_time = time.time()
        total_time = (end_time - start_time) * 1000
        
        # åˆ†æç»“æœ
        successful = [r for r in results if r.get("success")]
        failed = [r for r in results if not r.get("success")]
        
        if successful:
            avg_response_time = statistics.mean([r["response_time"] for r in successful])
            throughput = len(successful) / (total_time / 1000)  # requests per second
            
            print_metric("  æ€»è€—æ—¶", total_time, "ms")
            print_metric("  å¹³å‡å“åº”æ—¶é—´", avg_response_time, "ms", 1000)
            print_metric("  ååé‡", throughput, " req/s")
            print(f"  âœ… æˆåŠŸ: {len(successful)}/{concurrency}")
            
            if failed:
                print(f"  âŒ å¤±è´¥: {len(failed)}/{concurrency}")
        
        performance_results["concurrent_results"].append({
            "concurrency": concurrency,
            "total_time": total_time,
            "successful": len(successful),
            "failed": len(failed),
            "avg_response_time": avg_response_time if successful else 0,
            "throughput": throughput if successful else 0
        })


async def test_stress_scenario(client: httpx.AsyncClient):
    """å‹åŠ›æµ‹è¯•åœºæ™¯"""
    print_header("å‹åŠ›æµ‹è¯•")
    
    print("æ‰§è¡Œ30ç§’å‹åŠ›æµ‹è¯•...")
    
    start_time = time.time()
    request_count = 0
    error_count = 0
    response_times = []
    
    while time.time() - start_time < 30:  # è¿è¡Œ30ç§’
        try:
            req_start = time.time()
            response = await client.get(f"{BASE_URL}/health")
            req_end = time.time()
            
            request_count += 1
            response_times.append((req_end - req_start) * 1000)
            
            if response.status_code != 200:
                error_count += 1
                
        except Exception:
            error_count += 1
        
        # å°å»¶è¿Ÿé¿å…è¿‡åº¦å‹åŠ›
        await asyncio.sleep(0.1)
    
    total_time = time.time() - start_time
    
    if response_times:
        avg_response_time = statistics.mean(response_times)
        p95_response_time = statistics.quantiles(response_times, n=20)[18]  # 95th percentile
        throughput = request_count / total_time
        
        print(f"\nå‹åŠ›æµ‹è¯•ç»“æœ (è¿è¡Œæ—¶é—´: {total_time:.1f}ç§’):")
        print_metric("  æ€»è¯·æ±‚æ•°", request_count, "")
        print_metric("  é”™è¯¯æ•°", error_count, "")
        print_metric("  æˆåŠŸç‡", (1 - error_count/request_count) * 100, "%")
        print_metric("  å¹³å‡å“åº”æ—¶é—´", avg_response_time, "ms", 500)
        print_metric("  95%å“åº”æ—¶é—´", p95_response_time, "ms", 1000)
        print_metric("  å¹³å‡ååé‡", throughput, " req/s")


async def test_memory_usage():
    """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    print_header("èµ„æºä½¿ç”¨ç›‘æ§")
    
    try:
        import psutil
        import os
        
        # è·å–å½“å‰è¿›ç¨‹ä¿¡æ¯
        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()
        cpu_percent = process.cpu_percent(interval=1)
        
        print(f"æµ‹è¯•è¿›ç¨‹èµ„æºä½¿ç”¨:")
        print_metric("  å†…å­˜ä½¿ç”¨", memory_info.rss / 1024 / 1024, "MB")
        print_metric("  CPUä½¿ç”¨ç‡", cpu_percent, "%")
        
        # å°è¯•è·å–ç³»ç»Ÿæ•´ä½“ä¿¡æ¯
        system_memory = psutil.virtual_memory()
        system_cpu = psutil.cpu_percent(interval=1)
        
        print(f"\nç³»ç»Ÿæ•´ä½“èµ„æº:")
        print_metric("  ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡", system_memory.percent, "%")
        print_metric("  ç³»ç»ŸCPUä½¿ç”¨ç‡", system_cpu, "%")
        
    except ImportError:
        print("âš ï¸  psutilæœªå®‰è£…ï¼Œè·³è¿‡èµ„æºç›‘æ§")
    except Exception as e:
        print(f"âŒ èµ„æºç›‘æ§å¤±è´¥: {e}")


def generate_performance_report():
    """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
    print_header("ğŸ“Š æ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
    
    # å“åº”æ—¶é—´æ€»ç»“
    if performance_results["response_times"]:
        print("\nå“åº”æ—¶é—´æ€»ç»“:")
        for result in performance_results["response_times"]:
            status = "âœ…" if result["avg_time"] <= 2000 else "âš ï¸"
            print(f"{status} {result['endpoint']}: {result['avg_time']:.1f}ms")
    
    # å¹¶å‘æ€§èƒ½æ€»ç»“
    if performance_results["concurrent_results"]:
        print("\nå¹¶å‘æ€§èƒ½æ€»ç»“:")
        for result in performance_results["concurrent_results"]:
            success_rate = result["successful"] / (result["successful"] + result["failed"]) * 100
            print(f"  å¹¶å‘{result['concurrency']}: {result['throughput']:.1f} req/s, æˆåŠŸç‡ {success_rate:.1f}%")
    
    # æ€§èƒ½è¯„çº§
    avg_response_times = [r["avg_time"] for r in performance_results["response_times"]]
    if avg_response_times:
        overall_avg = statistics.mean(avg_response_times)
        
        if overall_avg <= 500:
            grade = "ğŸ† ä¼˜ç§€"
        elif overall_avg <= 1000:
            grade = "âœ… è‰¯å¥½"
        elif overall_avg <= 2000:
            grade = "âš ï¸  ä¸€èˆ¬"
        else:
            grade = "âŒ éœ€è¦ä¼˜åŒ–"
        
        print(f"\næ•´ä½“æ€§èƒ½è¯„çº§: {grade}")
        print(f"å¹³å‡å“åº”æ—¶é—´: {overall_avg:.1f}ms")
    
    # ä¼˜åŒ–å»ºè®®
    print(f"\nğŸ”§ ä¼˜åŒ–å»ºè®®:")
    
    slow_endpoints = [r for r in performance_results["response_times"] if r["avg_time"] > 1000]
    if slow_endpoints:
        print("  â€¢ ä»¥ä¸‹ç«¯ç‚¹å“åº”æ—¶é—´è¾ƒæ…¢ï¼Œå»ºè®®ä¼˜åŒ–:")
        for endpoint in slow_endpoints:
            print(f"    - {endpoint['endpoint']}: {endpoint['avg_time']:.1f}ms")
    
    high_error_endpoints = [r for r in performance_results["response_times"] if r["errors"] > 0]
    if high_error_endpoints:
        print("  â€¢ ä»¥ä¸‹ç«¯ç‚¹å­˜åœ¨é”™è¯¯ï¼Œéœ€è¦æ£€æŸ¥:")
        for endpoint in high_error_endpoints:
            print(f"    - {endpoint['endpoint']}: {endpoint['errors']} é”™è¯¯")
    
    if not slow_endpoints and not high_error_endpoints:
        print("  â€¢ ç³»ç»Ÿæ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œæ— éœ€ç‰¹åˆ«ä¼˜åŒ–")


async def main():
    """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
    print("\n" + "âš¡" * 20)
    print("DPAç³»ç»Ÿæ€§èƒ½æµ‹è¯•")
    print("âš¡" * 20)
    print(f"ç›®æ ‡åœ°å€: {BASE_URL}")
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    async with httpx.AsyncClient(timeout=30.0) as client:
        # 1. å“åº”æ—¶é—´æµ‹è¯•
        await test_response_times(client)
        
        # 2. å¹¶å‘æ€§èƒ½æµ‹è¯•
        await test_concurrent_requests(client)
        
        # 3. å‹åŠ›æµ‹è¯•
        await test_stress_scenario(client)
        
        # 4. èµ„æºä½¿ç”¨ç›‘æ§
        await test_memory_usage()
        
        # 5. ç”ŸæˆæŠ¥å‘Š
        generate_performance_report()
    
    print(f"\nç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")


if __name__ == "__main__":
    asyncio.run(main())