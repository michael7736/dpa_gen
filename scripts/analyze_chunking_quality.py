#!/usr/bin/env python3
"""
æ·±å…¥åˆ†æç¬¬äºŒæ­¥ä¼˜åŒ–çš„åˆ†å—è´¨é‡
"""
import asyncio
import sys
import re
from pathlib import Path
from collections import Counter
import statistics

sys.path.append(str(Path(__file__).parent.parent))

from src.database.qdrant_client import get_qdrant_client
from src.utils.logging_utils import get_logger
from src.core.document.mvp_document_processor import tiktoken_len

logger = get_logger(__name__)


async def analyze_chunk_quality(project_id: str, strategy_name: str):
    """åˆ†æåˆ†å—è´¨é‡"""
    print(f"\n{'='*60}")
    print(f"ğŸ“Š {strategy_name} åˆ†å—è´¨é‡åˆ†æ")
    print(f"{'='*60}")
    
    qdrant = get_qdrant_client()
    
    try:
        # è·å–æ‰€æœ‰åˆ†å—
        collection_name = f"project_{project_id}"
        points, _ = await qdrant.scroll_points(
            collection_name=collection_name,
            limit=100,
            with_payload=True,
            with_vectors=False
        )
        
        if not points:
            print("æœªæ‰¾åˆ°åˆ†å—æ•°æ®")
            return
            
        # åˆ†ææŒ‡æ ‡
        chunk_metrics = {
            'token_lengths': [],
            'char_lengths': [],
            'sentence_counts': [],
            'boundary_types': Counter(),
            'word_break_positions': [],
            'semantic_completeness': []
        }
        
        for point in points:
            content = point.payload.get('content', '')
            if not content:
                continue
                
            # Tokenå’Œå­—ç¬¦é•¿åº¦
            token_count = tiktoken_len(content)
            chunk_metrics['token_lengths'].append(token_count)
            chunk_metrics['char_lengths'].append(len(content))
            
            # å¥å­æ•°é‡
            sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿï¼›\.\?!])\s*', content)
            sentences = [s for s in sentences if s.strip()]
            chunk_metrics['sentence_counts'].append(len(sentences))
            
            # è¾¹ç•Œç±»å‹åˆ†æ
            last_20_chars = content[-20:].strip()
            if re.search(r'[ã€‚ï¼ï¼Ÿ\.\?!]$', last_20_chars):
                chunk_metrics['boundary_types']['å®Œæ•´å¥å­'] += 1
                chunk_metrics['semantic_completeness'].append(1.0)
            elif re.search(r'[ï¼›;]$', last_20_chars):
                chunk_metrics['boundary_types']['åˆ†å·ç»“å°¾'] += 1
                chunk_metrics['semantic_completeness'].append(0.8)
            elif re.search(r'[ï¼Œ,]$', last_20_chars):
                chunk_metrics['boundary_types']['é€—å·ç»“å°¾'] += 1
                chunk_metrics['semantic_completeness'].append(0.5)
            else:
                # æ£€æŸ¥æ˜¯å¦åœ¨å•è¯ä¸­æ–­
                if re.search(r'\w$', last_20_chars):
                    chunk_metrics['boundary_types']['è¯ä¸­æ–­'] += 1
                    chunk_metrics['semantic_completeness'].append(0.0)
                else:
                    chunk_metrics['boundary_types']['å…¶ä»–'] += 1
                    chunk_metrics['semantic_completeness'].append(0.3)
        
        # è¾“å‡ºåˆ†æç»“æœ
        print(f"\n1. åŸºæœ¬ç»Ÿè®¡ ({len(points)} ä¸ªåˆ†å—):")
        print(f"   Tokené•¿åº¦:")
        print(f"     - å¹³å‡: {statistics.mean(chunk_metrics['token_lengths']):.1f}")
        print(f"     - æ ‡å‡†å·®: {statistics.stdev(chunk_metrics['token_lengths']) if len(chunk_metrics['token_lengths']) > 1 else 0:.1f}")
        print(f"     - èŒƒå›´: {min(chunk_metrics['token_lengths'])} - {max(chunk_metrics['token_lengths'])}")
        
        print(f"\n   å­—ç¬¦é•¿åº¦:")
        print(f"     - å¹³å‡: {statistics.mean(chunk_metrics['char_lengths']):.1f}")
        print(f"     - ä¸­ä½æ•°: {statistics.median(chunk_metrics['char_lengths'])}")
        
        print(f"\n   å¥å­æ•°é‡:")
        print(f"     - å¹³å‡æ¯å—: {statistics.mean(chunk_metrics['sentence_counts']):.1f} å¥")
        print(f"     - èŒƒå›´: {min(chunk_metrics['sentence_counts'])} - {max(chunk_metrics['sentence_counts'])} å¥")
        
        print(f"\n2. è¯­ä¹‰å®Œæ•´æ€§åˆ†æ:")
        total_chunks = sum(chunk_metrics['boundary_types'].values())
        for boundary_type, count in chunk_metrics['boundary_types'].most_common():
            percentage = (count / total_chunks) * 100
            quality_emoji = "âœ…" if boundary_type == 'å®Œæ•´å¥å­' else "âš ï¸" if boundary_type in ['åˆ†å·ç»“å°¾', 'é€—å·ç»“å°¾'] else "âŒ"
            print(f"   {quality_emoji} {boundary_type}: {count} ({percentage:.1f}%)")
        
        avg_completeness = statistics.mean(chunk_metrics['semantic_completeness'])
        print(f"\n   ğŸ“ˆ å¹³å‡è¯­ä¹‰å®Œæ•´åº¦: {avg_completeness:.2%}")
        
        # å±•ç¤ºç¤ºä¾‹
        print(f"\n3. åˆ†å—ç¤ºä¾‹:")
        for i, point in enumerate(points[:2]):
            content = point.payload.get('content', '')
            print(f"\n   ç¤ºä¾‹ {i+1}:")
            print(f"   Tokenæ•°: {tiktoken_len(content)}")
            print(f"   å¼€å§‹: {content[:50]}...")
            print(f"   ç»“å°¾: ...{content[-50:]}")
            
    except Exception as e:
        print(f"   âŒ åˆ†æå¤±è´¥: {e}")


async def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*80)
    print("ğŸ”¬ åˆ†å—è´¨é‡æ·±åº¦åˆ†æ")
    print("="*80)
    
    # åˆ†æä¸¤ç§ç­–ç•¥çš„åˆ†å—è´¨é‡
    await analyze_chunk_quality("optimization_step1", "ç¬¬ä¸€æ­¥ä¼˜åŒ–ï¼ˆTokenåˆ†å—ï¼‰")
    await analyze_chunk_quality("optimization_step2", "ç¬¬äºŒæ­¥ä¼˜åŒ–ï¼ˆå¥å­åˆ†å—ï¼‰")
    
    print("\n" + "="*80)
    print("ğŸ’¡ å…³é”®å‘ç°")
    print("="*80)
    print("\nç¬¬äºŒæ­¥ä¼˜åŒ–çš„ä¸»è¦æ”¹è¿›ï¼š")
    print("1. å¤§å¹…æé«˜äº†å¥å­å®Œæ•´ç‡ï¼ˆä»è¯ä¸­æ–­åˆ°å¥å­ç»“å°¾ï¼‰")
    print("2. æ›´ç¨³å®šçš„åˆ†å—å¤§å°ï¼ˆæ ‡å‡†å·®æ›´å°ï¼‰")
    print("3. ä¿æŒäº†åˆç†çš„tokenæ•°é‡ï¼ˆæ¥è¿‘ç›®æ ‡512ï¼‰")
    print("4. å¥å­çº§åˆ«çš„é‡å ç¡®ä¿äº†ä¸Šä¸‹æ–‡è¿ç»­æ€§")
    print("\nè¿™äº›æ”¹è¿›ç›´æ¥æå‡äº†è¯­ä¹‰æ£€ç´¢çš„å‡†ç¡®æ€§ï¼")


if __name__ == "__main__":
    asyncio.run(main())