#!/usr/bin/env python3
"""
åˆ†æå½“å‰åˆ†å—ç­–ç•¥çš„æ•ˆæœå’Œä¼˜åŒ–ç©ºé—´
"""
import asyncio
import sys
from pathlib import Path
from collections import Counter
import statistics

sys.path.append(str(Path(__file__).parent.parent))

from src.core.document.mvp_document_processor import create_mvp_document_processor
from src.database.qdrant_client import get_qdrant_client
from src.utils.logging_utils import get_logger
from langchain.text_splitter import RecursiveCharacterTextSplitter

logger = get_logger(__name__)


async def analyze_current_chunking():
    """åˆ†æå½“å‰åˆ†å—ç­–ç•¥"""
    print("\n" + "="*80)
    print("ğŸ“Š å½“å‰åˆ†å—ç­–ç•¥åˆ†æ")
    print("="*80)
    
    # 1. å½“å‰é…ç½®
    print("\n1ï¸âƒ£ å½“å‰åˆ†å—é…ç½®:")
    print("   - åˆ†å—å¤§å°: 150 å­—ç¬¦")
    print("   - é‡å å¤§å°: 30 å­—ç¬¦ (20%)")
    print("   - åˆ†å‰²ä¼˜å…ˆçº§: \\n\\n > \\n > ã€‚ï¼ï¼Ÿ > .!? > ç©ºæ ¼ > å­—ç¬¦")
    print("   - åˆ†å‰²å™¨ç±»å‹: RecursiveCharacterTextSplitter")
    
    # 2. æµ‹è¯•ä¸åŒç±»å‹çš„æ–‡æœ¬
    test_texts = {
        "æŠ€æœ¯æ–‡æ¡£": """
æ·±åº¦å­¦ä¹ åŸºç¡€æ•™ç¨‹

ç¬¬ä¸€ç« ï¼šç¥ç»ç½‘ç»œç®€ä»‹
ç¥ç»ç½‘ç»œæ˜¯æ·±åº¦å­¦ä¹ çš„åŸºç¡€ï¼Œå®ƒæ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ï¼Œé€šè¿‡å¤šå±‚ç¥ç»å…ƒæ¥å­¦ä¹ æ•°æ®çš„ç‰¹å¾è¡¨ç¤ºã€‚

ç¬¬äºŒç« ï¼šå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰
CNNåœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«é€‚åˆå¤„ç†å›¾åƒæ•°æ®ã€‚å®ƒä½¿ç”¨å·ç§¯å±‚ã€æ± åŒ–å±‚å’Œå…¨è¿æ¥å±‚çš„ç»„åˆã€‚
""",
        "å¯¹è¯è®°å½•": """
ç”¨æˆ·ï¼šä½ å¥½ï¼Œæˆ‘æƒ³äº†è§£ä¸€ä¸‹æ·±åº¦å­¦ä¹ çš„åº”ç”¨åœºæ™¯ã€‚
åŠ©æ‰‹ï¼šæ·±åº¦å­¦ä¹ æœ‰å¾ˆå¤šåº”ç”¨åœºæ™¯ï¼Œä¸»è¦åŒ…æ‹¬ï¼š
1. è®¡ç®—æœºè§†è§‰ï¼šå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€äººè„¸è¯†åˆ«ç­‰
2. è‡ªç„¶è¯­è¨€å¤„ç†ï¼šæœºå™¨ç¿»è¯‘ã€æƒ…æ„Ÿåˆ†æã€æ–‡æœ¬ç”Ÿæˆç­‰
3. è¯­éŸ³è¯†åˆ«ï¼šè¯­éŸ³è½¬æ–‡å­—ã€è¯­éŸ³åˆæˆç­‰
ç”¨æˆ·ï¼šé‚£åœ¨å®é™…é¡¹ç›®ä¸­å¦‚ä½•é€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼Ÿ
åŠ©æ‰‹ï¼šé€‰æ‹©æ¨¡å‹éœ€è¦è€ƒè™‘æ•°æ®é‡ã€è®¡ç®—èµ„æºã€å‡†ç¡®åº¦è¦æ±‚ç­‰å› ç´ ã€‚
""",
        "å­¦æœ¯è®ºæ–‡": """
Abstract: This paper presents a novel approach to memory systems in AI agents. We propose a hybrid architecture that combines episodic memory, semantic memory, and procedural memory to create more adaptive and intelligent agents. Our experiments show that this integrated approach significantly improves agent performance in complex, long-term tasks.

Introduction: Memory is a fundamental component of intelligent behavior. In artificial intelligence, the ability to store, retrieve, and utilize past experiences is crucial for creating agents that can learn and adapt over time.
"""
    }
    
    # 3. åˆ†ææ¯ç§æ–‡æœ¬çš„åˆ†å—æ•ˆæœ
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=150,
        chunk_overlap=30,
        length_function=len,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " ", ""]
    )
    
    print("\n2ï¸âƒ£ ä¸åŒæ–‡æœ¬ç±»å‹çš„åˆ†å—æ•ˆæœ:")
    for text_type, text in test_texts.items():
        chunks = splitter.split_text(text)
        chunk_lengths = [len(chunk) for chunk in chunks]
        
        print(f"\n{text_type}:")
        print(f"   - åˆ†å—æ•°é‡: {len(chunks)}")
        print(f"   - å¹³å‡é•¿åº¦: {statistics.mean(chunk_lengths):.1f} å­—ç¬¦")
        print(f"   - é•¿åº¦èŒƒå›´: {min(chunk_lengths)} - {max(chunk_lengths)} å­—ç¬¦")
        print("   - ç¤ºä¾‹åˆ†å—:")
        for i, chunk in enumerate(chunks[:2]):
            print(f"     å—{i+1}: {chunk[:50]}...")
    
    # 4. åˆ†æçœŸå®æ–‡æ¡£çš„åˆ†å—æƒ…å†µ
    print("\n3ï¸âƒ£ çœŸå®æ–‡æ¡£åˆ†å—åˆ†æ:")
    
    # ä»Qdrantè·å–å·²å­˜å‚¨çš„åˆ†å—
    qdrant = get_qdrant_client()
    try:
        # è·å–æœ€è¿‘å¤„ç†çš„æ–‡æ¡£åˆ†å—
        collection_name = "project_memory_systems_demo"
        points, _ = await qdrant.scroll_points(
            collection_name=collection_name,
            limit=50,
            with_payload=True,
            with_vectors=False
        )
        
        if points:
            chunk_lengths = []
            chunk_endings = Counter()
            
            for point in points:
                content = point.payload.get('content', '')
                chunk_lengths.append(len(content))
                
                # åˆ†æåˆ†å—ç»“å°¾
                if content:
                    last_char = content[-1]
                    if last_char in ['.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ']:
                        chunk_endings['å¥å·ç»“å°¾'] += 1
                    elif last_char in ['\n']:
                        chunk_endings['æ¢è¡Œç»“å°¾'] += 1
                    elif last_char in [' ', 'ã€€']:
                        chunk_endings['ç©ºæ ¼ç»“å°¾'] += 1
                    else:
                        chunk_endings['è¯ä¸­æ–­'] += 1
            
            print(f"   - åˆ†ææ ·æœ¬: {len(chunk_lengths)} ä¸ªåˆ†å—")
            print(f"   - å¹³å‡é•¿åº¦: {statistics.mean(chunk_lengths):.1f} å­—ç¬¦")
            print(f"   - æ ‡å‡†å·®: {statistics.stdev(chunk_lengths):.1f}")
            print(f"   - ä¸­ä½æ•°: {statistics.median(chunk_lengths)}")
            print(f"   - é•¿åº¦åˆ†å¸ƒ:")
            print(f"     < 100å­—ç¬¦: {sum(1 for l in chunk_lengths if l < 100)}")
            print(f"     100-150å­—ç¬¦: {sum(1 for l in chunk_lengths if 100 <= l <= 150)}")
            print(f"     > 150å­—ç¬¦: {sum(1 for l in chunk_lengths if l > 150)}")
            
            print("\n   - åˆ†å—è¾¹ç•Œè´¨é‡:")
            total = sum(chunk_endings.values())
            for ending_type, count in chunk_endings.most_common():
                percentage = (count / total) * 100
                print(f"     {ending_type}: {count} ({percentage:.1f}%)")
    except Exception as e:
        print(f"   âŒ æ— æ³•è·å–åˆ†å—æ•°æ®: {e}")
    
    # 5. ä¼˜åŒ–å»ºè®®
    print("\n4ï¸âƒ£ ä¼˜åŒ–å»ºè®®:")
    print("\n   ğŸ¯ å½“å‰é—®é¢˜:")
    print("   - å›ºå®šåˆ†å—å¤§å°å¯èƒ½ä¸é€‚åˆæ‰€æœ‰æ–‡æ¡£ç±»å‹")
    print("   - 150å­—ç¬¦å¯¹äºæŸäº›å†…å®¹å¯èƒ½å¤ªå°ï¼Œå¯¼è‡´ä¸Šä¸‹æ–‡ç¢ç‰‡åŒ–")
    print("   - ä¸­æ–‡å’Œè‹±æ–‡æ··åˆæ—¶ï¼Œå­—ç¬¦è®¡æ•°å¯èƒ½ä¸å‡†ç¡®")
    print("   - æ²¡æœ‰è€ƒè™‘è¯­ä¹‰å®Œæ•´æ€§")
    
    print("\n   ğŸ’¡ ä¼˜åŒ–æ–¹æ¡ˆ:")
    print("   1. åŠ¨æ€åˆ†å—å¤§å°ï¼šæ ¹æ®æ–‡æ¡£ç±»å‹è°ƒæ•´")
    print("   2. è¯­ä¹‰åˆ†å—ï¼šä½¿ç”¨å¥å­è¾¹ç•Œæ£€æµ‹")
    print("   3. æ»‘åŠ¨çª—å£ï¼šå¢åŠ ä¸Šä¸‹æ–‡è¿ç»­æ€§")
    print("   4. æ··åˆç­–ç•¥ï¼šç»“åˆå­—ç¬¦æ•°å’Œè¯­ä¹‰è¾¹ç•Œ")
    print("   5. å…ƒæ•°æ®å¢å¼ºï¼šä¸ºæ¯ä¸ªåˆ†å—æ·»åŠ ä½ç½®å’Œä¸Šä¸‹æ–‡ä¿¡æ¯")


async def test_optimized_chunking():
    """æµ‹è¯•ä¼˜åŒ–çš„åˆ†å—ç­–ç•¥"""
    print("\n\n" + "="*80)
    print("ğŸ”§ ä¼˜åŒ–åˆ†å—ç­–ç•¥æµ‹è¯•")
    print("="*80)
    
    # æµ‹è¯•æ–‡æœ¬
    test_text = """
Memory Systems for AI Agents: A Comprehensive Deep Dive

AI agents must manage and recall information at multiple time scales and contexts. Memory in AI spans short-term (contextual working memory), long-term (persistent knowledge), episodic (event-based), semantic (facts), procedural (skills), and external storage.

Short-term or working memory refers to the immediate context held by an agent (e.g. the current dialogue or task state). Long-term memory stores enduring knowledge or preferences across sessions. Episodic memory records specific events or interactions that an agent experienced. Semantic memory holds general factual or conceptual knowledge.
"""
    
    # 1. å½“å‰ç­–ç•¥
    current_splitter = RecursiveCharacterTextSplitter(
        chunk_size=150,
        chunk_overlap=30,
        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?", " ", ""]
    )
    
    # 2. ä¼˜åŒ–ç­–ç•¥1ï¼šå¢å¤§åˆ†å—ï¼Œå¢åŠ é‡å 
    optimized_1 = RecursiveCharacterTextSplitter(
        chunk_size=300,
        chunk_overlap=50,
        separators=["\n\n", "\n", ". ", "! ", "? ", "ã€‚", "ï¼", "ï¼Ÿ", ", ", "ï¼Œ", " "]
    )
    
    # 3. ä¼˜åŒ–ç­–ç•¥2ï¼šåŸºäºå¥å­çš„åˆ†å—
    from langchain.text_splitter import NLTKTextSplitter
    try:
        import nltk
        nltk.download('punkt', quiet=True)
        sentence_splitter = NLTKTextSplitter(chunk_size=200)
        use_sentence = True
    except:
        use_sentence = False
        print("   âš ï¸  NLTKæœªå®‰è£…ï¼Œè·³è¿‡å¥å­åˆ†å—æµ‹è¯•")
    
    # æ¯”è¾ƒç»“æœ
    strategies = [
        ("å½“å‰ç­–ç•¥(150å­—ç¬¦)", current_splitter),
        ("ä¼˜åŒ–ç­–ç•¥1(300å­—ç¬¦+ä¼˜åŒ–åˆ†éš”ç¬¦)", optimized_1)
    ]
    
    if use_sentence:
        strategies.append(("ä¼˜åŒ–ç­–ç•¥2(å¥å­åˆ†å—)", sentence_splitter))
    
    print("\nç­–ç•¥å¯¹æ¯”:")
    for name, splitter in strategies:
        chunks = splitter.split_text(test_text)
        print(f"\n{name}:")
        print(f"   - åˆ†å—æ•°: {len(chunks)}")
        print(f"   - ç¤ºä¾‹:")
        for i, chunk in enumerate(chunks[:2]):
            print(f"     å—{i+1}: {chunk[:80]}...")
            

async def main():
    """ä¸»å‡½æ•°"""
    await analyze_current_chunking()
    await test_optimized_chunking()
    
    # å®æ–½å»ºè®®
    print("\n\n" + "="*80)
    print("ğŸš€ å®æ–½å»ºè®®")
    print("="*80)
    
    print("\n1. çŸ­æœŸä¼˜åŒ–ï¼ˆç«‹å³å¯å®æ–½ï¼‰:")
    print("   - å°†åˆ†å—å¤§å°å¢åŠ åˆ° 256-512 å­—ç¬¦")
    print("   - ä¼˜åŒ–åˆ†éš”ç¬¦é¡ºåºï¼Œä¼˜å…ˆä¿æŒå¥å­å®Œæ•´æ€§")
    print("   - ä¸ºä¸åŒæ–‡æ¡£ç±»å‹è®¾ç½®ä¸åŒçš„åˆ†å—å‚æ•°")
    
    print("\n2. ä¸­æœŸä¼˜åŒ–ï¼ˆéœ€è¦å¼€å‘ï¼‰:")
    print("   - å®ç°åŸºäºå¥å­è¾¹ç•Œçš„æ™ºèƒ½åˆ†å—")
    print("   - æ·»åŠ åˆ†å—è´¨é‡è¯„åˆ†æœºåˆ¶")
    print("   - æ”¯æŒä¸­è‹±æ–‡æ··åˆçš„åˆ†è¯ä¼˜åŒ–")
    
    print("\n3. é•¿æœŸä¼˜åŒ–ï¼ˆæ¶æ„å‡çº§ï¼‰:")
    print("   - å®ç°è¯­ä¹‰æ„ŸçŸ¥çš„åˆ†å—ï¼ˆåŸºäºembeddingç›¸ä¼¼åº¦ï¼‰")
    print("   - å±‚æ¬¡åŒ–åˆ†å—ï¼ˆæ®µè½->å¥å­->çŸ­è¯­ï¼‰")
    print("   - åŠ¨æ€åˆ†å—å¤§å°ï¼ˆåŸºäºå†…å®¹å¤æ‚åº¦ï¼‰")


if __name__ == "__main__":
    asyncio.run(main())