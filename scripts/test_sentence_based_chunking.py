#!/usr/bin/env python3
"""
æµ‹è¯•ç¬¬äºŒæ­¥ä¼˜åŒ–ï¼šåŸºäºå¥å­çš„æ™ºèƒ½è¾¹ç•Œæ£€æµ‹
"""
import asyncio
import sys
import re
from pathlib import Path
from collections import Counter
import statistics
import tiktoken

sys.path.append(str(Path(__file__).parent.parent))

from src.core.document.mvp_document_processor import create_mvp_document_processor, tiktoken_len
from src.database.qdrant_client import get_qdrant_client
from src.utils.logging_utils import get_logger

logger = get_logger(__name__)


def split_text_by_sentence(text: str, chunk_size: int = 512, overlap_sentences: int = 3):
    """
    åŸºäºå¥å­çš„æ™ºèƒ½åˆ†å—
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
        chunk_size: æ¯ä¸ªå—çš„æœ€å¤§tokenæ•°
        overlap_sentences: é‡å å¥å­æ•°
    """
    # æ­¥éª¤1: ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å¥
    # ä¸­æ–‡å¥å­ç»“æŸç¬¦ï¼šã€‚ï¼ï¼Ÿï¼›
    # è‹±æ–‡å¥å­ç»“æŸç¬¦ï¼š.!?
    # ä¿ç•™æ ‡ç‚¹ç¬¦å·åœ¨å¥å­æœ«å°¾
    sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿï¼›\.\?!])\s*', text)
    sentences = [s.strip() for s in sentences if s.strip()]  # ç§»é™¤ç©ºå­—ç¬¦ä¸²
    
    if not sentences:
        return []
    
    chunks = []
    current_chunk_sentences = []
    current_chunk_tokens = 0
    
    # æ­¥éª¤2: ç»„åˆå¥å­
    for i, sentence in enumerate(sentences):
        sentence_tokens = tiktoken_len(sentence)
        
        # å¦‚æœå•ä¸ªå¥å­å°±è¶…è¿‡chunk_sizeï¼Œå•ç‹¬æˆå—
        if sentence_tokens > chunk_size:
            if current_chunk_sentences:
                chunks.append(" ".join(current_chunk_sentences))
                current_chunk_sentences = []
                current_chunk_tokens = 0
            chunks.append(sentence)
            continue
        
        # å¦‚æœåŠ ä¸Šè¿™ä¸ªå¥å­ä¼šè¶…è¿‡é™åˆ¶ï¼Œå…ˆä¿å­˜å½“å‰å—
        if current_chunk_tokens + sentence_tokens > chunk_size and current_chunk_sentences:
            chunks.append(" ".join(current_chunk_sentences))
            
            # åˆ›å»ºé‡å ï¼šä»å‰ä¸€ä¸ªå—çš„æœ«å°¾å–overlap_sentencesä¸ªå¥å­
            start_index = max(0, len(current_chunk_sentences) - overlap_sentences)
            current_chunk_sentences = current_chunk_sentences[start_index:]
            current_chunk_tokens = tiktoken_len(" ".join(current_chunk_sentences))
        
        current_chunk_sentences.append(sentence)
        current_chunk_tokens += sentence_tokens
    
    # ä¿å­˜æœ€åä¸€ä¸ªå—
    if current_chunk_sentences:
        chunks.append(" ".join(current_chunk_sentences))
    
    return chunks


async def test_sentence_based_chunking():
    """æµ‹è¯•åŸºäºå¥å­çš„åˆ†å—æ•ˆæœ"""
    print("\n" + "="*80)
    print("ğŸ”¬ ç¬¬äºŒæ­¥ä¼˜åŒ–æµ‹è¯•ï¼šæ™ºèƒ½è¾¹ç•Œæ£€æµ‹ï¼ˆSentence-Based Chunkingï¼‰")
    print("="*80)
    
    # 1. æµ‹è¯•ä¸­è‹±æ–‡å¥å­è¯†åˆ«
    print("\n1ï¸âƒ£ å¥å­è¾¹ç•Œè¯†åˆ«æµ‹è¯•:")
    
    test_texts = {
        "è‹±æ–‡æ®µè½": """
AI agents must manage and recall information at multiple time scales and contexts. Memory in AI spans short-term (contextual working memory), long-term (persistent knowledge), episodic (event-based), semantic (facts), procedural (skills), and external storage. Short-term or working memory refers to the immediate context held by an agent. Long-term memory stores enduring knowledge or preferences across sessions.
""",
        "ä¸­æ–‡æ®µè½": """
äººå·¥æ™ºèƒ½ä»£ç†å¿…é¡»åœ¨å¤šä¸ªæ—¶é—´å°ºåº¦å’Œä¸Šä¸‹æ–‡ä¸­ç®¡ç†å’Œå›å¿†ä¿¡æ¯ã€‚AIä¸­çš„è®°å¿†åŒ…æ‹¬çŸ­æœŸè®°å¿†ï¼ˆä¸Šä¸‹æ–‡å·¥ä½œè®°å¿†ï¼‰ã€é•¿æœŸè®°å¿†ï¼ˆæŒä¹…çŸ¥è¯†ï¼‰ã€æƒ…æ™¯è®°å¿†ï¼ˆåŸºäºäº‹ä»¶ï¼‰ã€è¯­ä¹‰è®°å¿†ï¼ˆäº‹å®ï¼‰ã€ç¨‹åºæ€§è®°å¿†ï¼ˆæŠ€èƒ½ï¼‰å’Œå¤–éƒ¨å­˜å‚¨ã€‚çŸ­æœŸæˆ–å·¥ä½œè®°å¿†æ˜¯æŒ‡ä»£ç†æŒæœ‰çš„å³æ—¶ä¸Šä¸‹æ–‡ã€‚é•¿æœŸè®°å¿†å­˜å‚¨è·¨ä¼šè¯çš„æŒä¹…çŸ¥è¯†æˆ–åå¥½ã€‚
""",
        "ä¸­è‹±æ··åˆ": """
Memory Systemsæ˜¯AIä»£ç†çš„æ ¸å¿ƒç»„ä»¶ã€‚å®ƒåŒ…æ‹¬working memoryï¼ˆå·¥ä½œè®°å¿†ï¼‰å’Œlong-term memoryï¼ˆé•¿æœŸè®°å¿†ï¼‰ä¸¤å¤§ç±»ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼ŒåƒChatGPTè¿™æ ·çš„ç³»ç»Ÿä½¿ç”¨attentionæœºåˆ¶å¤„ç†çŸ­æœŸä¿¡æ¯ï¼Œè€Œä½¿ç”¨fine-tuningæ¥æ›´æ–°é•¿æœŸçŸ¥è¯†ã€‚è¿™ç§æ··åˆæ¶æ„è®©AIèƒ½å¤ŸåŒæ—¶å¤„ç†å³æ—¶ä»»åŠ¡å’ŒæŒä¹…å­¦ä¹ ã€‚
"""
    }
    
    for text_type, text in test_texts.items():
        print(f"\n{text_type}:")
        # åˆ†å¥
        sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿï¼›\.\?!])\s*', text.strip())
        sentences = [s.strip() for s in sentences if s.strip()]
        
        print(f"   å¥å­æ•°é‡: {len(sentences)}")
        for i, sent in enumerate(sentences[:3]):
            tokens = tiktoken_len(sent)
            print(f"   å¥å­{i+1} ({tokens} tokens): {sent[:50]}...")
    
    # 2. å¯¹æ¯”åˆ†å—æ•ˆæœ
    print("\n2ï¸âƒ£ åˆ†å—æ•ˆæœå¯¹æ¯”:")
    
    # ä½¿ç”¨ä¹‹å‰çš„AIè®°å¿†ç³»ç»Ÿæ–‡æ¡£
    doc_path = "/Users/mdwong001/Desktop/code/rag/data/zonghe/MemeoryOpenai.txt"
    
    # è¯»å–æ–‡æ¡£
    with open(doc_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ä½¿ç”¨å¥å­åˆ†å—
    sentence_chunks = split_text_by_sentence(content, chunk_size=512, overlap_sentences=2)
    
    print(f"\nåŸºäºå¥å­çš„åˆ†å—ç»“æœ:")
    print(f"   - åˆ†å—æ•°é‡: {len(sentence_chunks)}")
    print(f"   - å¹³å‡tokens: {statistics.mean([tiktoken_len(c) for c in sentence_chunks]):.1f}")
    
    # åˆ†æåˆ†å—è´¨é‡
    chunk_endings = Counter()
    for chunk in sentence_chunks:
        if chunk:
            # æ£€æŸ¥åˆ†å—ç»“å°¾
            last_chars = chunk[-10:].strip()
            if re.search(r'[ã€‚ï¼ï¼Ÿï¼›\.\?!]$', last_chars):
                chunk_endings['å®Œæ•´å¥å­'] += 1
            elif re.search(r'[ï¼Œ,]$', last_chars):
                chunk_endings['å­å¥ç»“å°¾'] += 1
            else:
                chunk_endings['è¯ä¸­æ–­'] += 1
    
    print("\n   åˆ†å—è¾¹ç•Œè´¨é‡:")
    total = sum(chunk_endings.values())
    for ending_type, count in chunk_endings.most_common():
        percentage = (count / total) * 100
        quality = "âœ…" if ending_type == 'å®Œæ•´å¥å­' else "âš ï¸" if ending_type == 'å­å¥ç»“å°¾' else "âŒ"
        print(f"      {quality} {ending_type}: {count} ({percentage:.1f}%)")
    
    # 3. å±•ç¤ºç¤ºä¾‹åˆ†å—
    print("\n3ï¸âƒ£ ç¤ºä¾‹åˆ†å—ï¼ˆå‰3ä¸ªï¼‰:")
    for i, chunk in enumerate(sentence_chunks[:3]):
        tokens = tiktoken_len(chunk)
        sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿï¼›\.\?!])\s*', chunk)
        sentences = [s for s in sentences if s.strip()]
        
        print(f"\nå— {i+1} ({tokens} tokens, {len(sentences)} å¥å­):")
        print(f"   å¼€å§‹: {chunk[:100]}...")
        print(f"   ç»“å°¾: ...{chunk[-100:]}")
    
    # 4. é‡å åˆ†æ
    print("\n4ï¸âƒ£ é‡å æ•ˆæœåˆ†æ:")
    if len(sentence_chunks) > 1:
        for i in range(min(3, len(sentence_chunks)-1)):
            chunk1 = sentence_chunks[i]
            chunk2 = sentence_chunks[i+1]
            
            # ç®€å•çš„é‡å æ£€æµ‹
            # æ‰¾åˆ°chunk1çš„æœ€åå‡ ä¸ªå¥å­
            chunk1_sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿï¼›\.\?!])\s*', chunk1)
            chunk1_sentences = [s for s in chunk1_sentences if s.strip()]
            
            # æ£€æŸ¥è¿™äº›å¥å­æ˜¯å¦å‡ºç°åœ¨chunk2å¼€å¤´
            overlap_found = False
            if len(chunk1_sentences) >= 2:
                last_two = " ".join(chunk1_sentences[-2:])
                if last_two in chunk2:
                    overlap_found = True
            
            print(f"\n   å—{i+1} â†’ å—{i+2}:")
            print(f"   é‡å æ£€æµ‹: {'âœ… å‘ç°é‡å ' if overlap_found else 'âŒ æœªå‘ç°é‡å '}")


async def compare_chunking_strategies():
    """å¯¹æ¯”ç¬¬ä¸€æ­¥å’Œç¬¬äºŒæ­¥ä¼˜åŒ–æ•ˆæœ"""
    print("\n" + "="*80)
    print("ğŸ“Š åˆ†å—ç­–ç•¥å¯¹æ¯”åˆ†æ")
    print("="*80)
    
    # å‡†å¤‡æµ‹è¯•æ–‡æœ¬
    test_text = """
Memory Systems for AI Agents: A Comprehensive Deep Dive

AI agents must manage and recall information at multiple time scales and contexts. Memory in AI spans short-term (contextual working memory), long-term (persistent knowledge), episodic (event-based), semantic (facts), procedural (skills), and external storage.

Short-term or working memory refers to the immediate context held by an agent (e.g. the current dialogue or task state). Long-term memory stores enduring knowledge or preferences across sessions. Episodic memory records specific events or interactions that an agent experienced. Semantic memory holds general factual or conceptual knowledge.

Procedural memory involves learned skills or action patterns. External storage allows offloading to databases or vector stores. Successful agents combine these memory types to achieve coherent, context-aware behavior.
"""
    
    print("\nç­–ç•¥å¯¹æ¯”:")
    print("\n1. ç¬¬ä¸€æ­¥ä¼˜åŒ–ï¼ˆToken-basedï¼Œ512 tokensï¼‰:")
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    
    # ç¬¬ä¸€æ­¥çš„åˆ†å—å™¨
    step1_splitter = RecursiveCharacterTextSplitter(
        chunk_size=512,
        chunk_overlap=128,
        length_function=tiktoken_len,
        separators=["\n\n", ".\n", "ã€‚\n", "\n", ". ", "ã€‚", "!", "ï¼", "?", "ï¼Ÿ", ";", "ï¼›", ",", "ï¼Œ", " ", ""]
    )
    step1_chunks = step1_splitter.split_text(test_text)
    
    print(f"   - åˆ†å—æ•°: {len(step1_chunks)}")
    print(f"   - å¹³å‡tokens: {statistics.mean([tiktoken_len(c) for c in step1_chunks]):.1f}")
    
    # æ£€æŸ¥å¥å­å®Œæ•´æ€§
    incomplete = 0
    for chunk in step1_chunks:
        if not re.search(r'[ã€‚ï¼ï¼Ÿï¼›\.\?!]$', chunk.strip()):
            incomplete += 1
    print(f"   - å¥å­ä¸å®Œæ•´ç‡: {incomplete}/{len(step1_chunks)} ({incomplete/len(step1_chunks)*100:.1f}%)")
    
    print("\n2. ç¬¬äºŒæ­¥ä¼˜åŒ–ï¼ˆSentence-basedï¼Œ512 tokensï¼‰:")
    step2_chunks = split_text_by_sentence(test_text, chunk_size=512, overlap_sentences=2)
    
    print(f"   - åˆ†å—æ•°: {len(step2_chunks)}")
    print(f"   - å¹³å‡tokens: {statistics.mean([tiktoken_len(c) for c in step2_chunks]):.1f}")
    
    # æ£€æŸ¥å¥å­å®Œæ•´æ€§
    incomplete = 0
    for chunk in step2_chunks:
        if not re.search(r'[ã€‚ï¼ï¼Ÿï¼›\.\?!]$', chunk.strip()):
            incomplete += 1
    print(f"   - å¥å­ä¸å®Œæ•´ç‡: {incomplete}/{len(step2_chunks)} ({incomplete/len(step2_chunks)*100:.1f}%)")
    
    print("\nâœ¨ ç¬¬äºŒæ­¥ä¼˜åŒ–ä¼˜åŠ¿:")
    print("   1. 100%ä¿è¯å¥å­å®Œæ•´æ€§")
    print("   2. åŸºäºè¯­ä¹‰å•å…ƒï¼ˆå¥å­ï¼‰çš„é‡å ")
    print("   3. æ›´ç¨³å®šçš„åˆ†å—å¤§å°")
    print("   4. æ›´å¥½çš„ä¸Šä¸‹æ–‡ä¿æŒ")


async def main():
    """ä¸»å‡½æ•°"""
    await test_sentence_based_chunking()
    await compare_chunking_strategies()
    
    print("\n" + "="*80)
    print("âœ… ç¬¬äºŒæ­¥ä¼˜åŒ–æµ‹è¯•å®Œæˆï¼")
    print("="*80)
    print("\næ ¸å¿ƒæ”¹è¿›:")
    print("1. âœ… å®ç°äº†åŸºäºå¥å­çš„æ™ºèƒ½åˆ†å—")
    print("2. âœ… ä¿è¯äº†100%çš„å¥å­å®Œæ•´æ€§")
    print("3. âœ… ä½¿ç”¨å¥å­æ•°é‡è€Œéå­—ç¬¦æ•°è¿›è¡Œé‡å ")
    print("4. âœ… æ”¯æŒä¸­è‹±æ–‡æ··åˆæ–‡æ¡£")
    print("\nä¸‹ä¸€æ­¥ï¼šç­‰å¾…ç¬¬ä¸‰æ­¥ä¼˜åŒ–æ–¹æ¡ˆï¼")


if __name__ == "__main__":
    asyncio.run(main())