#!/usr/bin/env python3
"""
æµ‹è¯•åŸºäºtiktokençš„åˆ†å—ä¼˜åŒ–æ•ˆæœ
éªŒè¯ç¬¬ä¸€æ­¥ä¼˜åŒ–ï¼šä½¿ç”¨tokenè®¡æ•°ä»£æ›¿å­—ç¬¦è®¡æ•°
"""
import asyncio
import sys
from pathlib import Path
from collections import Counter
import statistics

sys.path.append(str(Path(__file__).parent.parent))

from src.core.document.mvp_document_processor import create_mvp_document_processor, tiktoken_len
from src.database.qdrant_client import get_qdrant_client
from src.utils.logging_utils import get_logger

logger = get_logger(__name__)


async def test_tiktoken_chunking():
    """æµ‹è¯•åŸºäºtiktokençš„åˆ†å—æ•ˆæœ"""
    print("\n" + "="*80)
    print("ğŸ”¬ ç¬¬ä¸€æ­¥ä¼˜åŒ–æµ‹è¯•ï¼šåŸºäºTokençš„æ™ºèƒ½åˆ†å—")
    print("="*80)
    
    # 1. æµ‹è¯•ä¸­è‹±æ–‡tokenè®¡ç®—å·®å¼‚
    print("\n1ï¸âƒ£ Tokenè®¡ç®—å¯¹æ¯”ï¼ˆä¸­æ–‡ vs è‹±æ–‡ï¼‰:")
    
    test_cases = [
        ("Hello world", "çº¯è‹±æ–‡"),
        ("ä½ å¥½ä¸–ç•Œ", "çº¯ä¸­æ–‡"),
        ("AI agents must manage information", "è‹±æ–‡å¥å­"),
        ("äººå·¥æ™ºèƒ½ä»£ç†å¿…é¡»ç®¡ç†ä¿¡æ¯", "ä¸­æ–‡å¥å­"),
        ("Memory in AI spans short-term and long-term", "è‹±æ–‡é•¿å¥"),
        ("AIä¸­çš„è®°å¿†åŒ…æ‹¬çŸ­æœŸè®°å¿†å’Œé•¿æœŸè®°å¿†", "ä¸­è‹±æ··åˆ"),
    ]
    
    print(f"{'æ–‡æœ¬å†…å®¹':<40} {'ç±»å‹':<10} {'å­—ç¬¦æ•°':<8} {'Tokenæ•°':<8} {'æ¯”ä¾‹':<8}")
    print("-" * 80)
    
    for text, text_type in test_cases:
        char_count = len(text)
        token_count = tiktoken_len(text)
        ratio = token_count / char_count if char_count > 0 else 0
        print(f"{text:<40} {text_type:<10} {char_count:<8} {token_count:<8} {ratio:<8.2f}")
    
    # 2. æµ‹è¯•å®é™…æ–‡æ¡£å¤„ç†
    print("\n2ï¸âƒ£ å¤„ç†çœŸå®æ–‡æ¡£ï¼ˆä¼˜åŒ–åçš„åˆ†å—ï¼‰:")
    processor = create_mvp_document_processor()
    
    # ä½¿ç”¨ä¹‹å‰çš„AIè®°å¿†ç³»ç»Ÿæ–‡æ¡£
    doc_path = "/Users/mdwong001/Desktop/code/rag/data/zonghe/MemeoryOpenai.txt"
    
    # å¤„ç†æ–‡æ¡£
    result = await processor.process_document(
        file_path=doc_path,
        project_id="tiktoken_test"
    )
    
    print(f"\nâœ… æ–‡æ¡£å¤„ç†å®Œæˆ:")
    print(f"   - æ–‡æ¡£ID: {result.document_id}")
    print(f"   - åˆ†å—æ•°é‡: {result.chunk_count}")
    print(f"   - åˆ†å—ç­–ç•¥: 512 tokens/å—ï¼Œ128 tokensé‡å ")
    
    # ç­‰å¾…å­˜å‚¨å®Œæˆ
    await asyncio.sleep(3)
    
    # 3. åˆ†æåˆ†å—è´¨é‡
    print("\n3ï¸âƒ£ åˆ†å—è´¨é‡åˆ†æ:")
    
    qdrant = get_qdrant_client()
    try:
        # è·å–åˆ†å—æ•°æ®
        collection_name = "project_tiktoken_test"
        points, _ = await qdrant.scroll_points(
            collection_name=collection_name,
            limit=50,
            with_payload=True,
            with_vectors=False
        )
        
        if points:
            # åˆ†ætokenå’Œå­—ç¬¦é•¿åº¦
            token_lengths = []
            char_lengths = []
            chunk_endings = Counter()
            
            for point in points:
                content = point.payload.get('content', '')
                char_lengths.append(len(content))
                token_lengths.append(tiktoken_len(content))
                
                # åˆ†æåˆ†å—ç»“å°¾è´¨é‡
                if content:
                    # æ£€æŸ¥æœ€å10ä¸ªå­—ç¬¦
                    ending = content[-10:].strip()
                    if ending.endswith(('.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ')):
                        chunk_endings['å®Œæ•´å¥å­'] += 1
                    elif ending.endswith((',', 'ï¼Œ', ';', 'ï¼›')):
                        chunk_endings['å­å¥ç»“å°¾'] += 1
                    elif '\n' in ending:
                        chunk_endings['æ®µè½è¾¹ç•Œ'] += 1
                    else:
                        chunk_endings['è¯ä¸­æ–­'] += 1
            
            print(f"   ğŸ“Š Tokené•¿åº¦åˆ†æ:")
            print(f"      - å¹³å‡: {statistics.mean(token_lengths):.1f} tokens")
            print(f"      - æ ‡å‡†å·®: {statistics.stdev(token_lengths):.1f}")
            print(f"      - èŒƒå›´: {min(token_lengths)} - {max(token_lengths)} tokens")
            
            print(f"\n   ğŸ“ å­—ç¬¦é•¿åº¦åˆ†æ:")
            print(f"      - å¹³å‡: {statistics.mean(char_lengths):.1f} å­—ç¬¦")
            print(f"      - ä¸­ä½æ•°: {statistics.median(char_lengths)}")
            
            print(f"\n   âœ‚ï¸ åˆ†å—è¾¹ç•Œè´¨é‡:")
            total = sum(chunk_endings.values())
            for ending_type, count in chunk_endings.most_common():
                percentage = (count / total) * 100
                quality = "âœ…" if ending_type in ['å®Œæ•´å¥å­', 'æ®µè½è¾¹ç•Œ'] else "âš ï¸" if ending_type == 'å­å¥ç»“å°¾' else "âŒ"
                print(f"      {quality} {ending_type}: {count} ({percentage:.1f}%)")
            
            # æ˜¾ç¤ºç¤ºä¾‹åˆ†å—
            print(f"\n   ğŸ“„ ç¤ºä¾‹åˆ†å—ï¼ˆå‰3ä¸ªï¼‰:")
            for i, point in enumerate(points[:3]):
                content = point.payload.get('content', '').replace('\n', ' ').strip()
                tokens = tiktoken_len(content)
                chars = len(content)
                print(f"\n   å— {i+1} ({tokens} tokens, {chars} å­—ç¬¦):")
                print(f"   å¼€å§‹: {content[:100]}...")
                print(f"   ç»“å°¾: ...{content[-100:]}")
                
    except Exception as e:
        print(f"   âŒ æ— æ³•è·å–åˆ†å—æ•°æ®: {e}")
    
    # 4. å¯¹æ¯”ä¼˜åŒ–æ•ˆæœ
    print("\n4ï¸âƒ£ ä¼˜åŒ–æ•ˆæœæ€»ç»“:")
    print("\n   ğŸ¯ å…³é”®æ”¹è¿›:")
    print("   1. Tokenè®¡æ•°ç¡®ä¿ä¸­è‹±æ–‡å…¬å¹³å¤„ç†")
    print("   2. ä¼˜åŒ–çš„åˆ†éš”ç¬¦ä¼˜å…ˆçº§æé«˜è¯­ä¹‰å®Œæ•´æ€§")
    print("   3. æ›´å¤§çš„åˆ†å—å°ºå¯¸(512 tokens)é€‚åˆç ”ç©¶æ€§æ–‡æ¡£")
    print("   4. 25%é‡å (128 tokens)ä¿è¯ä¸Šä¸‹æ–‡è¿ç»­æ€§")
    
    print("\n   ğŸ“ˆ é¢„æœŸæ•ˆæœ:")
    print("   - æ›´å°‘çš„è¯­ä¹‰å‰²è£‚")
    print("   - æ›´é«˜çš„æ£€ç´¢ç²¾åº¦")
    print("   - æ›´å¥½çš„ä¸­è‹±æ–‡æ··åˆæ”¯æŒ")
    print("   - æ›´ç¨³å®šçš„åˆ†å—è´¨é‡")


async def main():
    """ä¸»å‡½æ•°"""
    await test_tiktoken_chunking()
    
    print("\n" + "="*80)
    print("âœ… ç¬¬ä¸€æ­¥ä¼˜åŒ–å®Œæˆï¼")
    print("=" * 80)
    print("\næ ¸å¿ƒæ”¹è¿›å·²å®æ–½:")
    print("1. âœ… ä½¿ç”¨tiktokenæ›¿ä»£len()è¿›è¡Œé•¿åº¦è®¡ç®—")
    print("2. âœ… ä¼˜åŒ–åˆ†éš”ç¬¦é¡ºåºï¼ˆæ®µè½>å¥å­>æ ‡ç‚¹>ç©ºæ ¼ï¼‰")
    print("3. âœ… å¢å¤§åˆ†å—å°ºå¯¸åˆ°512 tokens")
    print("4. âœ… è®¾ç½®25%é‡å ç¡®ä¿ä¸Šä¸‹æ–‡è¿ç»­")
    print("\nå‡†å¤‡å¥½è¿›è¡Œç¬¬äºŒæ­¥ä¼˜åŒ–äº†ï¼")


if __name__ == "__main__":
    asyncio.run(main())