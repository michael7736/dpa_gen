"""
æ–‡æ¡£ç®¡ç†APIè·¯ç”± V2
æ”¯æŒç”¨æˆ·é€‰æ‹©å¤„ç†é€‰é¡¹çš„æ–°ç‰ˆæœ¬
"""

import asyncio
from typing import Dict, Any, List, Optional
from uuid import UUID, uuid4
from datetime import datetime

from fastapi import APIRouter, HTTPException, UploadFile, File, BackgroundTasks, Depends, status, Form
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from sqlalchemy.orm import Session

from ...database.postgresql import get_db_session
from ...models.document import Document, ProcessingStatus, DocumentType
from ...models.processing_pipeline import ProcessingPipeline, PipelineStage, StageStatus, ProcessingStage
from ...services.project_service import ProjectService
from ...services.minio_service import get_minio_service
from ...services.pipeline_executor import PipelineExecutor, get_pipeline_executor
from ...utils.logger import get_logger
from ..middleware.auth import get_current_user

logger = get_logger(__name__)
router = APIRouter(prefix="/api/v2/documents", tags=["documents-v2"])


def _get_document_type(file_extension: str) -> str:
    """æ ¹æ®æ–‡ä»¶æ‰©å±•åè·å–æ–‡æ¡£ç±»å‹"""
    extension_map = {
        '.pdf': DocumentType.PDF,
        '.docx': DocumentType.WORD,
        '.doc': DocumentType.WORD,
        '.txt': DocumentType.TEXT,
        '.md': DocumentType.MARKDOWN
    }
    return extension_map.get(file_extension, DocumentType.TEXT)


class ProcessingOptions(BaseModel):
    """å¤„ç†é€‰é¡¹"""
    upload_only: bool = Field(True, description="ä»…ä¸Šä¼ ")
    generate_summary: bool = Field(False, description="ç”Ÿæˆæ‘˜è¦")
    create_index: bool = Field(False, description="åˆ›å»ºç´¢å¼•")
    deep_analysis: bool = Field(False, description="æ·±åº¦åˆ†æ")


class DocumentUploadResponse(BaseModel):
    """æ–‡æ¡£ä¸Šä¼ å“åº”"""
    document_id: str
    filename: str
    size: int
    status: str
    message: str
    processing_pipeline: Optional[Dict[str, Any]] = None


class PipelineProgressResponse(BaseModel):
    """ç®¡é“è¿›åº¦å“åº”"""
    pipeline_id: str
    overall_progress: float
    current_stage: Optional[str]
    stages: List[Dict[str, Any]]
    can_resume: bool
    interrupted: bool


@router.post("/upload", response_model=DocumentUploadResponse)
async def upload_document_v2(
    file: UploadFile = File(...),
    project_id: Optional[str] = Form(None),
    upload_only: bool = Form(True),
    generate_summary: bool = Form(False),
    create_index: bool = Form(False),
    deep_analysis: bool = Form(False),
    background_tasks: BackgroundTasks = BackgroundTasks(),
    db: Session = Depends(get_db_session),
    current_user: str = Depends(get_current_user)
):
    """
    ä¸Šä¼ æ–‡æ¡£å¹¶æ ¹æ®ç”¨æˆ·é€‰æ‹©å¤„ç†
    
    Args:
        file: ä¸Šä¼ çš„æ–‡ä»¶
        project_id: é¡¹ç›®IDï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨é»˜è®¤é¡¹ç›®ï¼‰
        upload_only: ä»…ä¸Šä¼ 
        generate_summary: ç”Ÿæˆæ‘˜è¦
        create_index: åˆ›å»ºç´¢å¼•
        deep_analysis: æ·±åº¦åˆ†æ
        
    Returns:
        æ–‡æ¡£ä¸Šä¼ å“åº”ï¼ŒåŒ…å«å¤„ç†ç®¡é“ä¿¡æ¯
    """
    try:
        # è®°å½•å¤„ç†é€‰é¡¹
        logger.info(f"å¤„ç†é€‰é¡¹: upload_only={upload_only}, generate_summary={generate_summary}, "
                   f"create_index={create_index}, deep_analysis={deep_analysis}")
        
        # å¤„ç†ç”¨æˆ·IDæ ¼å¼ - å•ç”¨æˆ·é˜¶æ®µå…¼å®¹æ€§
        user_uuid = None
        if current_user == "u1":
            # å•ç”¨æˆ·é˜¶æ®µï¼Œæ˜ å°„åˆ°é»˜è®¤ç”¨æˆ·UUID
            user_uuid = UUID("243588ff-459d-45b8-b77b-09aec3946a64")
        else:
            try:
                # å°è¯•è§£æä¸ºUUID
                user_uuid = UUID(current_user)
            except ValueError:
                # å¦‚æœä¸æ˜¯æœ‰æ•ˆUUIDï¼Œä½¿ç”¨é»˜è®¤ç”¨æˆ·
                user_uuid = UUID("243588ff-459d-45b8-b77b-09aec3946a64")
        
        # å¦‚æœæ²¡æœ‰æä¾›é¡¹ç›®IDæˆ–è€…é¡¹ç›®IDæ˜¯"default"ï¼Œä½¿ç”¨ç”¨æˆ·çš„é»˜è®¤é¡¹ç›®
        if not project_id or project_id == "default":
            project_service = ProjectService(db)
            default_project = await project_service.get_or_create_default_project(user_uuid)
            project_id = str(default_project.id)
            logger.info(f"ä½¿ç”¨ç”¨æˆ· {current_user} çš„é»˜è®¤é¡¹ç›®: {project_id}")
        
        # éªŒè¯æ–‡ä»¶ç±»å‹
        allowed_types = ['.pdf', '.docx', '.doc', '.txt', '.md']
        file_extension = '.' + file.filename.split('.')[-1].lower()
        
        if file_extension not in allowed_types:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_extension}. æ”¯æŒçš„ç±»å‹: {', '.join(allowed_types)}"
            )
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = await file.read()
        file_size = len(content)
        
        # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
        import hashlib
        file_hash = hashlib.sha256(content).hexdigest()
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒå“ˆå¸Œçš„æ–‡æ¡£
        existing_doc = db.query(Document).filter(
            Document.file_hash == file_hash,
            Document.project_id == UUID(project_id)
        ).first()
        
        if existing_doc:
            # å¦‚æœæ–‡æ¡£å·²å­˜åœ¨ï¼Œè¿”å›ç°æœ‰æ–‡æ¡£ä¿¡æ¯
            logger.info(f"æ–‡æ¡£å·²å­˜åœ¨ï¼Œè¿”å›ç°æœ‰æ–‡æ¡£: {existing_doc.id}")
            return DocumentUploadResponse(
                document_id=str(existing_doc.id),
                filename=existing_doc.filename,
                size=existing_doc.file_size,
                status="exists",
                message=f"æ–‡æ¡£å·²å­˜åœ¨ï¼š{existing_doc.filename}",
                processing_pipeline=None
            )
        
        # ç”Ÿæˆæ–‡æ¡£ID
        document_id = str(uuid4())
        
        # ä¸Šä¼ åˆ°MinIO
        minio_service = get_minio_service()
        storage_path = await minio_service.upload_document(
            file_content=content,
            file_name=file.filename,
            user_id=current_user,
            project_id=project_id,
            document_id=document_id,
            content_type=file.content_type,
            metadata={
                "file_size": str(file_size),
                "upload_time": datetime.utcnow().isoformat()
            }
        )
        
        # åˆ›å»ºæ–‡æ¡£è®°å½•
        document = Document(
            id=UUID(document_id),
            project_id=UUID(project_id),
            title=file.filename,
            filename=file.filename,
            file_path=storage_path,
            file_size=file_size,
            file_hash=file_hash,
            document_type=_get_document_type(file_extension),
            processing_status=ProcessingStatus.UPLOADED,
            language="zh",  # TODO: è‡ªåŠ¨æ£€æµ‹è¯­è¨€
            metadata_info={
                "storage_path": storage_path,
                "upload_user": current_user,
                "upload_time": datetime.utcnow().isoformat()
            }
        )
        
        db.add(document)
        
        # åˆ›å»ºå¤„ç†ç®¡é“ï¼ˆå¦‚æœéœ€è¦å¤„ç†ï¼‰
        pipeline_data = None
        if not upload_only:
            # åˆ›å»ºç®¡é“
            pipeline = ProcessingPipeline(
                document_id=UUID(document_id),
                user_id=user_uuid,
                processing_options={
                    "generate_summary": generate_summary,
                    "create_index": create_index,
                    "deep_analysis": deep_analysis
                }
            )
            db.add(pipeline)
            db.flush()  # è·å–pipeline.id
            
            # åˆ›å»ºå¤„ç†é˜¶æ®µ
            stages = []
            
            # ä¸Šä¼ é˜¶æ®µï¼ˆå·²å®Œæˆï¼‰
            upload_stage = PipelineStage(
                pipeline_id=pipeline.id,
                stage_type=ProcessingStage.UPLOAD,
                stage_name="æ–‡ä»¶ä¸Šä¼ ",
                status=StageStatus.COMPLETED,
                progress=100,
                completed_at=datetime.utcnow(),
                result={
                    "minio_path": storage_path,
                    "file_size": file_size
                }
            )
            stages.append(upload_stage)
            
            # æ‘˜è¦é˜¶æ®µ
            if generate_summary:
                summary_stage = PipelineStage(
                    pipeline_id=pipeline.id,
                    stage_type=ProcessingStage.SUMMARY,
                    stage_name="ç”Ÿæˆæ‘˜è¦",
                    status=StageStatus.PENDING,
                    estimated_time=30
                )
                stages.append(summary_stage)
            
            # ç´¢å¼•é˜¶æ®µ
            if create_index:
                index_stage = PipelineStage(
                    pipeline_id=pipeline.id,
                    stage_type=ProcessingStage.INDEX,
                    stage_name="åˆ›å»ºç´¢å¼•",
                    status=StageStatus.PENDING,
                    estimated_time=120
                )
                stages.append(index_stage)
            
            # åˆ†æé˜¶æ®µ
            if deep_analysis:
                analysis_stage = PipelineStage(
                    pipeline_id=pipeline.id,
                    stage_type=ProcessingStage.ANALYSIS,
                    stage_name="æ·±åº¦åˆ†æ",
                    status=StageStatus.PENDING,
                    estimated_time=300
                )
                stages.append(analysis_stage)
            
            # æ‰¹é‡æ·»åŠ é˜¶æ®µ
            for stage in stages:
                db.add(stage)
            
            db.commit()
            
            # å‡†å¤‡ç®¡é“å“åº”æ•°æ®
            pipeline_data = {
                "pipeline_id": str(pipeline.id),
                "stages": [
                    {
                        "id": stage.stage_type,
                        "name": stage.stage_name,
                        "status": stage.status,
                        "progress": stage.progress,
                        "estimated_time": stage.estimated_time,
                        "result": stage.result
                    }
                    for stage in stages
                ]
            }
            
            # å¯åŠ¨åå°å¤„ç†
            if len(stages) > 1:  # æœ‰é™¤äº†ä¸Šä¼ ä¹‹å¤–çš„é˜¶æ®µ
                background_tasks.add_task(
                    execute_pipeline_background,
                    pipeline_id=str(pipeline.id),
                    db_session=db
                )
        else:
            db.commit()
        
        logger.info(f"æ–‡æ¡£ä¸Šä¼ æˆåŠŸ: {file.filename} ({file_size} bytes)")
        
        return DocumentUploadResponse(
            document_id=document_id,
            filename=file.filename,
            size=file_size,
            status="uploaded",
            message="æ–‡æ¡£ä¸Šä¼ æˆåŠŸ" + ("ï¼Œå¤„ç†å·²å¼€å§‹" if not upload_only else ""),
            processing_pipeline=pipeline_data
        )
        
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        logger.error(f"æ–‡æ¡£ä¸Šä¼ å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æ–‡æ¡£ä¸Šä¼ å¤±è´¥: {str(e)}"
        )


async def execute_pipeline_background(pipeline_id: str, db_session: Session):
    """åå°æ‰§è¡Œå¤„ç†ç®¡é“ - æ”¹è¿›ç‰ˆæœ¬"""
    import asyncio
    from ..websocket import get_progress_notifier
    
    logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œç®¡é“: {pipeline_id}")
    
    try:
        # è·å–æ‰§è¡Œå™¨
        executor = get_pipeline_executor()
        
        # è®¾ç½®è¶…æ—¶ï¼ˆ10åˆ†é’Ÿï¼‰
        timeout = 600
        
        # æ‰§è¡Œç®¡é“ï¼ˆå¸¦è¶…æ—¶æ§åˆ¶ï¼‰
        await asyncio.wait_for(
            executor.execute(pipeline_id, db_session),
            timeout=timeout
        )
        
        logger.info(f"âœ… ç®¡é“æ‰§è¡Œå®Œæˆ: {pipeline_id}")
        
        # å‘é€å®Œæˆé€šçŸ¥
        try:
            notifier = get_progress_notifier()
            await notifier.notify_pipeline_progress(pipeline_id, db_session)
            logger.info(f"ğŸ“¡ WebSocketé€šçŸ¥å·²å‘é€: {pipeline_id}")
        except Exception as ws_error:
            logger.warning(f"WebSocketé€šçŸ¥å¤±è´¥: {ws_error}")
            
    except asyncio.TimeoutError:
        logger.error(f"â° ç®¡é“æ‰§è¡Œè¶…æ—¶: {pipeline_id} (è¶…æ—¶: {timeout}ç§’)")
        
        # æ ‡è®°ç®¡é“ä¸ºä¸­æ–­
        try:
            pipeline = db_session.query(ProcessingPipeline).filter(
                ProcessingPipeline.id == pipeline_id
            ).first()
            if pipeline:
                pipeline.interrupted = True
                pipeline.completed_at = datetime.utcnow()
                db_session.commit()
                logger.info(f"ğŸš« ç®¡é“å·²æ ‡è®°ä¸ºä¸­æ–­: {pipeline_id}")
        except Exception as db_error:
            logger.error(f"æ ‡è®°ç®¡é“ä¸­æ–­æ—¶å‡ºé”™: {db_error}")
            
    except Exception as e:
        logger.error(f"âŒ ç®¡é“æ‰§è¡Œå¼‚å¸¸: {pipeline_id} - {e}")
        import traceback
        logger.error(f"å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
        
        # æ ‡è®°ç®¡é“ä¸ºå¤±è´¥
        try:
            pipeline = db_session.query(ProcessingPipeline).filter(
                ProcessingPipeline.id == pipeline_id
            ).first()
            if pipeline:
                pipeline.interrupted = True
                pipeline.completed_at = datetime.utcnow()
                db_session.commit()
                logger.info(f"ğŸš« ç®¡é“å·²æ ‡è®°ä¸ºå¤±è´¥: {pipeline_id}")
        except Exception as db_error:
            logger.error(f"æ ‡è®°ç®¡é“å¤±è´¥æ—¶å‡ºé”™: {db_error}")
    
    finally:
        # ç¡®ä¿æ•°æ®åº“ä¼šè¯å…³é—­
        try:
            db_session.close()
        except:
            pass


@router.get("/{document_id}/pipeline/{pipeline_id}/progress", response_model=PipelineProgressResponse)
async def get_pipeline_progress(
    document_id: str,
    pipeline_id: str,
    db: Session = Depends(get_db_session),
    current_user: str = Depends(get_current_user)
):
    """è·å–å¤„ç†ç®¡é“è¿›åº¦"""
    try:
        # éªŒè¯æ–‡æ¡£å’Œç®¡é“
        pipeline = db.query(ProcessingPipeline).filter(
            ProcessingPipeline.id == pipeline_id,
            ProcessingPipeline.document_id == document_id,
            ProcessingPipeline.user_id == current_user
        ).first()
        
        if not pipeline:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="ç®¡é“ä¸å­˜åœ¨"
            )
        
        # è·å–æ‰€æœ‰é˜¶æ®µ
        stages = db.query(PipelineStage).filter(
            PipelineStage.pipeline_id == pipeline.id
        ).order_by(PipelineStage.id).all()
        
        # æ„å»ºå“åº”
        stage_data = []
        for stage in stages:
            stage_info = {
                "id": stage.stage_type,
                "name": stage.stage_name,
                "status": stage.status,
                "progress": stage.progress,
                "message": stage.message,
                "can_interrupt": stage.can_interrupt
            }
            
            if stage.started_at:
                stage_info["started_at"] = stage.started_at.isoformat()
            if stage.completed_at:
                stage_info["completed_at"] = stage.completed_at.isoformat()
                stage_info["duration"] = stage.duration
            if stage.error:
                stage_info["error"] = stage.error
                
            stage_data.append(stage_info)
        
        return PipelineProgressResponse(
            pipeline_id=str(pipeline.id),
            overall_progress=pipeline.overall_progress,
            current_stage=pipeline.current_stage,
            stages=stage_data,
            can_resume=pipeline.can_resume,
            interrupted=pipeline.interrupted
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–ç®¡é“è¿›åº¦å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="è·å–ç®¡é“è¿›åº¦å¤±è´¥"
        )


@router.post("/{document_id}/pipeline/{pipeline_id}/interrupt")
async def interrupt_pipeline(
    document_id: str,
    pipeline_id: str,
    db: Session = Depends(get_db_session),
    current_user: str = Depends(get_current_user)
):
    """ä¸­æ–­å¤„ç†ç®¡é“"""
    try:
        # éªŒè¯å¹¶è·å–ç®¡é“
        pipeline = db.query(ProcessingPipeline).filter(
            ProcessingPipeline.id == pipeline_id,
            ProcessingPipeline.document_id == document_id,
            ProcessingPipeline.user_id == current_user
        ).first()
        
        if not pipeline:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="ç®¡é“ä¸å­˜åœ¨"
            )
        
        if pipeline.completed:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="ç®¡é“å·²å®Œæˆï¼Œæ— æ³•ä¸­æ–­"
            )
        
        # è®¾ç½®ä¸­æ–­æ ‡å¿—
        pipeline.interrupted = True
        pipeline.can_resume = True
        
        # æ›´æ–°å½“å‰å¤„ç†ä¸­çš„é˜¶æ®µ
        current_stage = db.query(PipelineStage).filter(
            PipelineStage.pipeline_id == pipeline.id,
            PipelineStage.status == StageStatus.PROCESSING
        ).first()
        
        if current_stage:
            current_stage.status = StageStatus.INTERRUPTED
            current_stage.message = "ç”¨æˆ·ä¸­æ–­å¤„ç†"
        
        db.commit()
        
        # TODO: é€šçŸ¥æ‰§è¡Œå™¨ä¸­æ–­
        
        return {"message": "å¤„ç†å·²ä¸­æ–­", "pipeline_id": str(pipeline.id)}
        
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        logger.error(f"ä¸­æ–­ç®¡é“å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="ä¸­æ–­ç®¡é“å¤±è´¥"
        )


@router.post("/{document_id}/pipeline/{pipeline_id}/resume")
async def resume_pipeline(
    document_id: str,
    pipeline_id: str,
    background_tasks: BackgroundTasks,
    db: Session = Depends(get_db_session),
    current_user: str = Depends(get_current_user)
):
    """æ¢å¤å¤„ç†ç®¡é“"""
    try:
        # éªŒè¯å¹¶è·å–ç®¡é“
        pipeline = db.query(ProcessingPipeline).filter(
            ProcessingPipeline.id == pipeline_id,
            ProcessingPipeline.document_id == document_id,
            ProcessingPipeline.user_id == current_user
        ).first()
        
        if not pipeline:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="ç®¡é“ä¸å­˜åœ¨"
            )
        
        if not pipeline.interrupted:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="ç®¡é“æœªè¢«ä¸­æ–­"
            )
        
        if not pipeline.can_resume:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="ç®¡é“æ— æ³•æ¢å¤"
            )
        
        # æ¸…é™¤ä¸­æ–­æ ‡å¿—
        pipeline.interrupted = False
        
        # é‡ç½®è¢«ä¸­æ–­çš„é˜¶æ®µ
        interrupted_stage = db.query(PipelineStage).filter(
            PipelineStage.pipeline_id == pipeline.id,
            PipelineStage.status == StageStatus.INTERRUPTED
        ).first()
        
        if interrupted_stage:
            interrupted_stage.status = StageStatus.PENDING
            interrupted_stage.message = "ç­‰å¾…æ¢å¤å¤„ç†"
        
        db.commit()
        
        # é‡æ–°å¯åŠ¨åå°å¤„ç†
        background_tasks.add_task(
            execute_pipeline_background,
            pipeline_id=str(pipeline.id),
            db_session=db
        )
        
        return {"message": "å¤„ç†å·²æ¢å¤", "pipeline_id": str(pipeline.id)}
        
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        logger.error(f"æ¢å¤ç®¡é“å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="æ¢å¤ç®¡é“å¤±è´¥"
        )